

[
  
  
    
    
      {
        "title": "Interconexión entre Bases de Datos",
        "excerpt": "Las interconexiones de servidores de bases de datos trata de acceder a datos que no están almacenados en nuestra base de datos, pudiendo combinarlos con los que ya tenemos.\n",
        "content": "\n\nLas interconexiones de servidores de bases de datos trata de acceder a datos que no están almacenados en nuestra base de datos, pudiendo combinarlos con los que ya tenemos.\n\nLas interconexiones de servidores de bases de datos son operaciones que pueden ser muy útiles en diferentes contextos. Básicamente, se trata de acceder a datos que no están almacenados en nuestra base de datos, pudiendo combinarlos con los que ya tenemos.\n\nEn esta práctica veremos varias formas de crear un enlace entre distintos servidores de bases de datos.\n\nLos servidores enlazados siempre tendrán que estar instalados en máquinas diferentes.\n\n1. Enlace ORACLE - ORACLE\n\nRealizar un enlace entre dos servidores de bases de datos ORACLE, explicando la configuración necesaria en ambos extremos y demostrando su funcionamiento.\n\nPara enlazar un Cliente Oracle con dirección 172.22.44.76/16 a un Servidor Oracle con dirección 172.22.7.59/16, tenemos que realizar unas configuraciones en cada una de las partes para que realice el enlace. Estas configuraciones se explicarán a continuación:\n\nConfiguración Servidor Oracle\n\nEn el Servidor 2 tendremos que crear un usuario con los privilegios que deseemos, ya que accedemos a través de dicho usuario, desde el cliente, a la base de datos. En este caso utilizaremos el usuario paloma que tiene privilegios para ver sus tablas creadas.\n\nConfiguramos el fichero /opt/oracle/product/12.2.0.1/dbhome_1/network/admin/listener.ora para escuchar en una determinada dirección y un determinado puerto.\n\n# listener.ora Network Configuration File: /opt/oracle/product/12.2.0.1/dbhome_1/network/admin/listener.$\n# Generated by Oracle configuration tools.\n\nSID_LIST_LISTENER =\n (SID_LIST =\n  (SID_DESC =\n   (GLOBAL_DBNAME = orcl)\n   (ORACLE_HOME = /opt/oracle/product/12.2.0.1/dbhome_1)\n   (SID_NAME = orcl)\n  )\n )\n\nLISTENER=\n (DESCRIPTION_LIST =\n  (DESCRIPTION =\n   (ADDRESS_LIST =\n    (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))\n   )\n   (ADDRESS_LIST =\n    (ADDRESS = (PROTOCOL = TCP)(HOST = 172.22.44.76)(PORT = 1521))\n   )\n  )\n )\n\n\n\n  LISTENER: Donde especificamos las direcciones y puertos desde donde se pueden conectar remotamente a nuestro servidor.\n\n  SID_LIST_LISTENER: Donde se indican los nombres de los servicios.\n\n\nConfiguración Cliente Oracle\n\nVamos a modificar el fichero /opt/oracle/product/12.2.0.1/dbhome_1/network/admin/tnsnames.ora.\n\n\n  \n    Si no esta en la dirección indicada, tendremos que crearlo nosostros.\n  \n\n\nsudo nano /opt/oracle/product/12.2.0.1/dbhome_1/network/admin/tnsnames.ora\n\n\nAñadimos las siguientes lineas en dicho fichero con las caracteristicas que hacen referencia al Servidor Oracle al que nos vamos a enlazar y añadimos la información refente a nuestro servidor si no teniamos el fichero creado.\n\nLISTENER_ORCL =\n (ADDRESS = (PROTOCOL = TCP)(HOST = localhost)(PORT = 1521))\n\nORCL =\n (DESCRIPTION = Mi Servidor Oracle\n    (ADDRESS = (PROTOCOL = TCP)(HOST = localhost)(PORT = 1521))\n    (CONNECT_DATA =\n        (SERVER = DEDICATED)\n        (SERVICE_NAME = orcl)\n    )\n )\n\nOraclePaloma =\n (DESCRIPTION = Servidor Oracle de Paloma\n    (ADDRESS = (PROTOCOL = TCP)(HOST = 172.22.7.59)(PORT = 1521))\n    (CONNECT_DATA =\n        (SERVER = DEDICATED)\n        (SERVICE_NAME = orcl)\n    )\n )\n\n\n\n  ADDRESS: Especificamos el protocolo, la dirección y el puerto de la máquina que queremos conectarnos.\n\n  SERVER = DEDICATED: Para crear un proceso del servidor para atender solo a la conexión indicada\n\n  SERVICE_NAME: Especificamos el nombre del sevicio de la máquina que queremos conectarnos. Este se especifica en el parámetro CONNECT_DATA.\n\n\n\n  \n    Ahora tenemos que reiniciar el servicio para que se apliquen los cambios:\n  \n\n\nlsnrctl stop\nlsnrctl start\n\n\n\n  \n    Nos devuelve el siguiente mensaje al reiniciar el listener:\n  \n\n\nEl servicio \"orcl\" tiene 1 instancia(s).\n  La instancia \"orcl\", con estado UNKNOWN, tiene 1 manejador(es) para este servicio...\n\n\nAsignamos privilegios al usuario moralg de nuestra base de datos, para que pueda crear el enlace.\n\nGRANT CREATE DATABASE LINK to moralg;\n\n\nYa solo queda crear un enlace en nuestro Servidor Oracle para que pueda enlazarse al Servidor Oracle 2. Para crear el link se realiza de la siguiente forma:\n\nCREATE DATABASE LINK ConexionPaloma\nCONNECT TO paloma\nIDENTIFIED BY paloma\nUSING 'orcl';\n\n\n\n  \n    Indicamos el nombre del link ConexionPaloma para que se conecte al usuario paloma, con la contraseña paloma usando el nombre del servicio orcl\n  \n\n\nComprobamos el enlace, realizando un SELECT a una de la tablas de paloma y combinandola con una de moralg:\n\n\n\n\n  \n    La consulta nos muestra el campo DESCRPCION de la tabla ASPECTOS al hacer la conexión por el enlace ConexionPaloma combinando con el campo NOMBREEQUIPOS de la tabla PALMARES.\n  \n\n\n2. Enlace POSTGRES - POSTGRES\n\nRealizar un enlace entre dos servidores de bases de datos Postgres, explicando la configuración necesaria en ambos extremos y demostrando su funcionamiento.\n\nVamos a realizar un enlace entre un Servidor de Postgres con la dirección 192.168.43.141/24 y un Cliente de Postgres con la dirección 192.168.43.66/24\n\nConfiguración del Servidor Postgres\n\nModificamos el fichero /etc/postgresql/11/main/postgresql.conf, descomentando una línea y añadiendo la dirección del cliente que quiere crear el enlace para que pueda escucharlo.\n\nlisten_addresses = '192.168.43.66, localhost'\n\n\nReiniciamos el servicio del servidor:\n\nsudo systemctl restart postgresql.service\n\n\nConfiguración del Cliente Postgres\n\nVamos a intalar el paquete postgresql-contrib para poder utilizar los módulos con CREATE EXTENSION\n\nsudo apt install postgresql-contrib\n\n\nAhora vamos a añadir un nuevo registro de autentificación en el fichero /etc/postgresql/11/main/pg_hba.conf indicandole la dirección del servidor y el tipo de autentificación md5 para todas las base de datos y usuarios.\n\nLa linea que hay que añadir es:\n\n# TYPE  DATABASE        USER            ADDRESS                 METHOD\n\nhost    all             all             192.168.43.66/24        md5\n\n\nReiniciamos el servicio del cliente:\n\nsudo systemctl restart postgresql.service\n\n\nPara crear el enlace vamos a utilizar el módulo dblink\n\nCREATE EXTENSION dblink;\n    CREATE EXTENSION\n\n\n\n  \n    Solo los usuarios con superusuarios pueden crear extensiones. Este privilegio se asigna de con ALTER ROLE &lt;name_role&gt; WITH superuser;\n  \n\n\nAhora podemos realizar una consulta a una base de datos del Servidor de Postgres con lo siguiente:\n\n\n\n\n  dblink: Tenemos que indicarle, para realizar el enlace, varios parámetros:\n\n  dbname: Nombre de la base de datos del servidor.\n\n  host: Direción del servidor.\n\n  user: Usuario con el cual nos queremos conectar al servidor.\n\n  select .....: Indicar una consulta.\n\n\n\n  \n    Además tenemos que indicar el tipo da datos de las columnas, ya que es obligatorio.\n  \n\n\nPodemos utilizar dblink_connect y le indicamos los parámetros del servidor para realizar una conexión persistente con el servidor y no tener que indicar más, durante la sesión, dichos parámetros.\n\nSELECT dblink_connect('ConexionPaloma', 'dbname=restaurante host=172.22.3.28 user=paloma password=paloma');\n\n\n\n  ConexionPaloma: Nombre que se le asigna a la conexión persistente.\n\n\nAhora realizamos una consulta con lo indicado anteriormente:\n\n\n\n3. Enlace ORACLE - POSTGRES\n\nRealizar un enlace entre un cliente ORACLE y un servidor Postgres, empleando Heterogeneus Services, explicando la configuración necesaria en ambos extremos y demostrando su funcionamiento.\n\n3.1 Enlace de Cliente Oracle a Servidor Postgres\n\n\nTenemos que configurar un enlace utilizando el driver ODBC (Open DataBase Connector) que es un estandar para el acceso a los datos de cualquier gestor de base de datos.\n\nConfiguración el Servidor Postgres\n\nModificamos el fichero /etc/postgresql/11/main/postgresql.conf, descomentando una línea y añadiendo la dirección del cliente que quiere crear el enlace para que pueda escucharlo.\n\nlisten_addresses = '192.168.43.66, localhost'\n\n\nAhora vamos a añadir un nuevo registro de autentificación en el fichero /etc/postgresql/11/main/pg_hba.conf indicandole la dirección del cliente y el tipo de autentificación md5 para todas las base de datos y usuarios.\n\nLa linea que hay que añadir es:\n\n# TYPE  DATABASE        USER            ADDRESS                 METHOD\n\nhost    all             all             192.168.43.66/24        md5\n\n\nReiniciamos el servicio del servidor:\n\nsudo systemctl restart postgresql.service\n\n\nConfiguración del Cliente Oracle\n\nInstalamos el driver ODBC:\n\nsudo apt install odbc-postgresql unixodbc\n\n\nSe nos creará por defecto, al instalar los paquetes anteriores, el fichero /etc/odbcinst.ini donde se indicará los drivers de ODBC que vamos a utilizar.\n\n[PostgreSQL ANSI]\nDescription=PostgreSQL ODBC driver (ANSI version)\nDriver=psqlodbca.so\nSetup=libodbcpsqlS.so\nDebug=0\nCommLog=1\nUsageCount=1\n\n[PostgreSQL Unicode]\nDescription=PostgreSQL ODBC driver (Unicode version)\nDriver=psqlodbcw.so\nSetup=libodbcpsqlS.so\nDebug=0\nCommLog=1\nUsageCount=1\n\n\nAhora tenemos que añadir al fichero /etc/odbc.ini los parametros de conexión del Servidor de Postgres.\n\n[PSQLA]\nDebug = 0\nCommLog = 0\nReadOnly = 1\nDriver = PostgreSQL ANSI\nServername = 192.168.43.66\nUsername = paloma\nPassword = paloma\nPort = 5432\nDatabase = restaurante\nTrace = 0\nTraceFile = /tmp/sql.log\n\n[PSQLU]\nDebug = 0\nCommLog = 0\nReadOnly = 0\nDriver = PostgreSQL Unicode\nServername = 192.168.43.66\nUsername = paloma\nPassword = paloma\nPort = 5432\nDatabase = restaurante\nTrace = 0\nTraceFile = /tmp/sql.log\n\n[Default]\nDriver = /usr/lib/x86_64-linux-gnu/odbc/liboplodbcS.so\n\n\n\n  Driver: Indicamos el driver, PostgreSQL ANSI y PostgreSQL Unicode, que hemos añadido al fichero /etc/odbcinst.ini.\n\n  Servername: Dirección del Servidor de Postgres.\n\n  Username: Nombre del usuario que vamos a utilixar para realizar la conexión al servidor.\n\n  Password: Contraseña del usuario paloma.\n\n  Port: Puerto del Servidor de Postgres.\n\n  Database: Base de datos a la que vamos a conectarnos.\n\n\nPodemos comprobar la configuración de los drivers y la configuración de la conexión de dichos drivers:\n\nodbcinst -q -d\n    [PostgreSQL ANSI]\n    [PostgreSQL Unicode]\n\n\nodbcinst -q -s\n    [PSQLA]\n    [PSQLU]\n    [Default]\n\n\nComprobamos una conexión al Servidor de Postgres:\n\nroot@servidororacle:/home/oracle# isql -v PSQLU\n+---------------------------------------+\n| Connected!                            |\n|                                       |\n| sql-statement                         |\n| help [tablename]                      |\n| quit                                  |\n|                                       |\n+---------------------------------------+\nSQL&gt; select * from aspectos;\n+-------+---------------------------------------------------+------------+\n| codigo| descripcion                                       | importancia|\n+-------+---------------------------------------------------+------------+\n| COL   | Color                                             | Baja       |\n| TEX   | Textura                                           | Alta       |\n| VOL   | Volumen                                           | Media      |\n| CAN   | Cantidad                                          | Alta       |\n| PRE   | Presentacion                                      | Alta       |\n| TEC   | Tecnica                                           | Media      |\n| ORI   | Originalidad                                      | media      |\n+-------+---------------------------------------------------+------------+\nSQLRowCount returns 7\n7 rows fetched\n\n\nAhora vamos a configurar el servicio de Heterogeneus Services, para esto vamos a crear el fichero /opt/oracle/product/12.2.0.1/dbhome_1/hs/admin/initPSQLU.ora y añadimos lo siguiente:\n\nHS_FDS_CONNECT_INFO = PSQLU\nHS_FDS_TRACE_LEVEL = Debug\nHS_FDS_SHAREABLE_NAME = /usr/lib/x86_64-linux-gnu/odbc/psqlodbcw.so\nHS_LANGUAGE = AMERICAN_AMERICA.WE8ISO8859P1\nset ODBCINI=/etc/odbc.ini\n\n\n\n  HS_FDS_CONNECT_INFO: Para indicar un nombre al servicio.\n\n  HS_FDS_TRACE_LEVEL: Para activar el servicio.\n\n  HS_FDS_SHAREABLE_NAME: Donde especificamos la dirección del driver configurado anteriormente\n\n  HS_LANGUAGE: Indicamos el idioma por defecto\n\n  set ODBCINI: Donde especificamos el fichero de configuración del driver para PSQL.\n\n\nConfiguramos el fichero /opt/oracle/product/12.2.0.1/dbhome_1/network/admin/listener.ora para que pueda escuchar en el driver de ODBC, añadiendo otra entrada al apartado de SID_DESC:\n\nSID_LIST_LISTENER =\n (SID_LIST =\n  (SID_DESC =\n   (GLOBAL_DBNAME = orcl)\n   (ORACLE_HOME = /opt/oracle/product/12.2.0.1/dbhome_1)\n   (SID_NAME = orcl)\n  )\n  (SID_DESC =\n    (SID_NAME = PSQLU)\n    (PROGRAM = dg4odbc)\n    (ORACLE_HOME = /opt/oracle/product/12.2.0.1/dbhome_1)\n  )\n )\n\n\n\n  SID_NAME: Le especificamos el nombre del servicio de ‘Heterogeneus Services’.\n\n  PROGRAM: Le indicamos el programa por defecto.\n\n\nReiniciamos el servicio:\n\nlsnrctl stop\nlsnrctl start\n\n\nConfiguramos el fichero /opt/oracle/product/12.2.0.1/dbhome_1/network/admin/tnsnames.ora creando un entrada para realizar la conexión al driver ODBC. Añadimos al fichero lo siguiente:\n\nPSQLU =\n (DESCRIPTION=\n (ADDRESS=(PROTOCOL=tcp)(HOST=localhost)(PORT=1521))\n   (CONNECT_DATA=(SID=PSQLU))\n   (HS=OK)\n )\n\n\n\n  Le indicamos que SID=PSQLU ya que es el nombre que le hemos asignado anteriormente y HS=OK para que utilice el servicio Heterogeneus Services.\n\n\nNos devuelve el siguiente mensaje al reiniciar el listener:\n\nEl servicio \"PSQLU\" tiene 1 instancia(s).\n  La instancia \"PSQLU\", con estado UNKNOWN, tiene 1 manejador(es) para este servicio...\n\n\nAsignamos privilegios al usuario moralg de nuestra base de datos, para que pueda crear el enlace.\n\nGRANT CREATE PUBLIC DATABASE LINK to moralg;\n\n\nAhora solo nos queda crear el enlace con CREATE LINK y realizar la consulta.\n\nCREATE PUBLIC DATABASE LINK ConexionPalomaPSQLU\nCONNECT TO \"paloma\"\nIDENTIFIED BY \"paloma\"\nUSING 'PSQLU';\n\n\n\n  \n    Indicamos el nombre del link ConexionPalomaPSQLU para que se conecte al usuario paloma, con la contraseña paloma usando el nombre del servicio PSQLU\n  \n\n\nComprobamos el enlace, realizando un SELECT a una de la tablas de paloma:\n\ncol nombreequipo format a30;\ncol descripcion format a30;\n\n\n\n  \n    Para que salga las columnas mas pequeñas y estéticas, utilizamos col &lt;nombre_campo&gt; format a&lt;tamaño&gt;.\n  \n\n\n\n\n3.2 Enlace de Cliente Postgres a Servidor Oracle\n\n\nPara realizar el enlace de Postgres a Oracle necesitamos el cliente de Oracle y descargamos la extensión para postgres oracle_fdw.\n\nConfiguración el Cliente Postgres\n\nDescargamos el cliente de oracle con wget desde la página oficial de oracle.\n\nwget https://download.oracle.com/otn_software/linux/instantclient/195000/oracle-instantclient19.5-basic-19.5.0.0.0-1.x86_64.rpm\n\n\nDescargamos dependencias y paquetes necesarios.\n\nwget https://download.oracle.com/otn_software/linux/instantclient/195000/oracle-instantclient19.5-sqlplus-19.5.0.0.0-1.x86_64.rpm\n\nwget https://download.oracle.com/otn_software/linux/instantclient/195000/oracle-instantclient19.5-tools-19.5.0.0.0-1.x86_64.rpm\n\nwget https://download.oracle.com/otn_software/linux/instantclient/195000/oracle-instantclient19.5-devel-19.5.0.0.0-1.x86_64.rpm\n\n\n\n  \n    Solo con el cliente de Oracle valdría para realizar el enlace, pero durante el proceso de esta tarea, nos surgieron errores de falta de ficheros que se crean automaticamente con las dependencias indicadas anteriormentes.\n  \n\n\nYa tenemos descargados los ficheros ahora tenemos que instalarlos, pero tenemos el problema que son .rpm.\nTenemos que utilizar el paquete de alien para pasarlos a .deb.\n\nInstalamos el paquete:\nsudo apt install alien\n\n\nConvertimos los .rpm a .deb:\nsudo alien -d oracle-instantclient19.5-basic-19.5.0.0.0-1.x86_64.rpm\nsudo alien -d oracle-instantclient19.5-sqlplus-19.5.0.0.0-1.x86_64.rpm\nsudo alien -d oracle-instantclient19.5-tools-19.5.0.0.0-1.x86_64.rpm\nsudo alien -d oracle-instantclient19.5-devel-19.5.0.0.0-1.x86_64.rpm\n\n\nInstalamos los paquetes con dpkg -i\nsudo dpkg -i oracle-instantclient19.5-basic_19.5.0.0.0-2_amd64.deb\nsudo dpkg -i oracle-instantclient19.5-sqlplus-19.5.0.0.0-1.x86_64.deb\nsudo dpkg -i oracle-instantclient19.5-tools-19.5.0.0.0-1.x86_64.deb\nsudo dpkg -i oracle-instantclient19.5-devel-19.5.0.0.0-1.x86_64.deb\n\n\nAhora vamos a descargarnos e instalarnos la extensión para postgres oracle_fdw.\nPara descargadnos oracle_fdw vamos a clonar un repositorio de Github.\ngit clone https://github.com/laurenz/oracle_fdw.git\n\n\nAhora tenemos que realizar la compilación de dicho paquete, para esto vamos a utlizar make\n\nPuede que se produzcan varios fallos durante la compilación, os mostraré los fallos que me saltaron durante la realización de esta tarea.\n\n\n\nFALLO 1\nvagrant@Postgres:~/oracle_fdw$ make\nMakefile:20: /usr/lib/postgresql/11/lib/pgxs/src/makefiles/pgxs.mk: No such file or directory\nmake: *** No rule to make target '/usr/lib/postgresql/11/lib/pgxs/src/makefiles/pgxs.mk'.  Stop.\n\n\nEste fallo se produce al no tener instalado el paquete postgresql-server-dev-all\n\nvagrant@Postgres:~/oracle_fdw$ sudo apt install postgresql-server-dev-all\n\n\n\nFALLO 2\noracle_utils.c:22:10: fatal error: oci.h: No such file or directory\n #include &lt;oci.h&gt;\n          ^~~~~~~\ncompilation terminated.\nmake: *** [&lt;builtin&gt;: oracle_utils.o] Error 1\n\nEste fallo se solucciona igualando las siguiente variables a las rutas correctas.\nexport ORACLE_HOME=\"/usr/lib/oracle/19.5/client64\"\nexport LD_LIBRARY_PATH=\"/usr/lib/oracle/19.5/client64/lib\"\nexport PATH=$ORACLE_HOME:$PATH\nexport USE_PGXS=1\n\n\n\nFALLO 3\n\nTenemos que modificar el Makefile, ya que la versión del cliente que nos hemos descargado es la 19.5 y esta no esta incluida en el Makefile:\n\nTenemos que añadir -I/usr/include/oracle/19.5/client64 en la variable PG_CPPFLAGS y tenemos que añadir -L/usr/lib/oracle/19.5/client64/lib en la variable SHLIB_LINK\n\nMakeFile corregido para la versión 19.5\nMODULE_big = oracle_fdw\nOBJS = oracle_fdw.o oracle_utils.o oracle_gis.o\nEXTENSION = oracle_fdw\nDATA = oracle_fdw--1.1.sql oracle_fdw--1.0--1.1.sql\nDOCS = README.oracle_fdw\nREGRESS = oracle_fdw oracle_gis oracle_import oracle_join\n\n# add include and library paths for both Instant Client and regular Client\nPG_CPPFLAGS = -I$(ORACLE_HOME)/sdk/include -I$(ORACLE_HOME)/oci/include -I$(ORACLE_HOME)/rdbms/public -I/usr/include/oracle/19.5/client64\n\nSHLIB_LINK = -L$(ORACLE_HOME) -L$(ORACLE_HOME)/bin -L$(ORACLE_HOME)/lib -L$(ORACLE_HOME)/lib/amd64 -l$(ORACLE_SHLIB) -L/usr/lib/oracle/19.5/client64/lib\n\nifdef NO_PGXS\nsubdir = contrib/oracle_fdw\ntop_builddir = ../..\ninclude $(top_builddir)/src/Makefile.global\ninclude $(top_srcdir)/contrib/contrib-global.mk\nelse\nPG_CONFIG = pg_config\nPGXS := $(shell $(PG_CONFIG) --pgxs)\ninclude $(PGXS)\nendif\n\n# Oracle's shared library is oci.dll on Windows and libclntsh elsewhere\nifeq ($(PORTNAME),win32)\nORACLE_SHLIB=oci\nelse\nifeq ($(PORTNAME),cygwin)\nORACLE_SHLIB=oci\nelse\nORACLE_SHLIB=clntsh\nendif\nendif\n\n\n\nAhora vamos a realizar la instalación, si todo ha salido bien en la compilación.\nsudo make install\n\n\nUna vez instalado la extensión, vamos a crearla en la base de datos.\n\nAccedemos a la base de datos con el usuario postgres y creamos la extensión con CREATE EXTENSION oracle_fdw.\npostgres@Postgres:/home/vagrant/oracle_fdw$ psql\npsql (11.5 (Debian 11.5-1+deb10u1))\nType \"help\" for help.\n\npostgres=# CREATE EXTENSION oracle_fdw;\nERROR:  could not load library \"/usr/lib/postgresql/11/lib/oracle_fdw.so\": libaio.so.1: cannot open shared object file: No such file or directory\n\n\n\n  \n    Nos salta un error al crear la extension. Esto se solucciona con la instalacción de dos librerías libaio1 y libaio-dev.\n  \n\n\nsudo apt-get install libaio1 libaio-dev\n\n\nCreamos la extensión:\nCREATE EXTENSION oracle_fdw;\n\n\nCreamos un nuevo servidor con la opciones del servidor de Oracle.\n\nCREATE SERVER ConexionOraclePaloma FOREIGN DATA WRAPPER oracle_fdw OPTIONS(dbserver '//192.168.43.58:1521/ORCL');\n\n\n\n  CREATE SERVER: define un nuevo servidor externo. El usuario que define el servidor se convierte en su propietario.\n\n\nLe asignamos el usuario paloma con la contraseña paloma.\n\nCREATE USER MAPPING for postgres SERVER ConexionOraclePaloma OPTIONS(user 'paloma', \n                                                                      password 'paloma');\n\n\nAhora tenemos que crear una tabla externa que debería de ser igual a la tabla del servidor Oracle que queremos consultar.\n\n\n  \n    Podemos realizar, en el servidor Oracle, la consulta DESCRIBE &lt;nombre_tabla&gt;; para que nos muestre la información necesario para crear la FOREIGN TABLE.\n  \n\n\nCreamos la tabla externa:\n\nCREATE FOREIGN TABLE ASPECTOS(codigo varchar(10), \n                              descripcion varchar(30), \n                              importancia varchar(10)) \nSERVER ConexionOraclePaloma OPTIONS(schema 'PALOMA', \n                                     table 'ASPECTOS');\n\n\n\n  \n    Hay que tener en cuenta que las opciones schema y table tienen que estar en mayúsculas, ya que oracle los guarda de este modo.\n  \n\n\nAhora realizamos la consulta:\n\n\n\n\n  \n    Si queremos dar permisos a un usuario concreto de Postgres para que pueda realizar consultas a un servidor externo concreto utilizaremos GRANT USAGE ON FOREIGN SERVER &lt;nombre_servidor&gt; TO &lt;nombre_usuario&gt;;.\n  \n\n",
        "url": "/base%20de%20datos/2019/12/27/Interconexion_de_Servidores_de_BBDD/"
      },
    
      {
        "title": "Cortafuego Perimetral con DMZ",
        "excerpt": "Trabajamos mejorando un cortafuego perimetral tradicional, para que sea mas seguro y añadiendo un equipo que haga de DMZ.\n",
        "content": "\n\nTrabajamos mejorando un cortafuego perimetral tradicional, para que sea mas seguro y añadiendo un equipo que haga de DMZ.\n\nEsquema de red\n\n\nVamos a utilizar tres máquinas en openstack, que vamos a crear con la receta heat: escenario3.yaml. La receta heat ha deshabilitado el cortafuego que nos ofrece openstack (todos los puertos de todos los protocolos están abiertos). Una máquina (que tiene asignada una IP flotante) hará de cortafuegos, otra será una máquina de la red interna 192.168.100.0/24 y la tercera será un servidor en la DMZ donde iremos instalando distintos servicios y estará en la red 192.168.200.0/24.\n\n\n\nCumplimientos\n\n\n\n  \n    Política por defecto DROP para las cadenas INPUT, FORWARD y OUTPUT.\n  \n\n\nLimpiamos las tablas.\n\nsudo iptables -F\nsudo iptables -t nat -F\nsudo iptables -Z\nsudo iptables -t nat -Z\n\n\nConexión ssh antes de estableces la politica por defecto en Drop para no perder la coonexión.\n\nsudo iptables -A INPUT -s 172.22.0.0/16 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A OUTPUT -d 172.22.0.0/16 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT\n\nsudo iptables -A INPUT -s 172.23.0.0/16 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A OUTPUT -d 172.23.0.0/16 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT\n\nsudo iptables -A OUTPUT -p tcp -o eth1 -d 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A INPUT -p tcp -i eth1 -s 192.168.100.0/24 --sport 22 -m state --state ESTABLISHED -j ACCEPT\n\nsudo iptables -A OUTPUT -p tcp -o eth2 -d 192.168.200.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A INPUT -p tcp -i eth2 -s 192.168.200.0/24 --sport 22 -m state --state ESTABLISHED -j ACCEPT\n\n\nEstablecemos la política.\n\nsudo iptables -P INPUT DROP\nsudo iptables -P OUTPUT DROP\nsudo iptables -P FORWARD DROP\n\n\n\n  Se pueden usar las extensiones que queremos adecuadas, pero al menos debe implementarse seguimiento de la conexión.\n  Debes indicar pruebas de funcionamiento de todos las reglas.\n\n\nTareas\n\n\nTarea 1. La máquina router-fw tiene un servidor ssh escuchando por el puerto 22, pero al acceder desde el exterior habrá que conectar al puerto 2222.\n\nReglas\n\nHay que activar el ‘ip_forward’\nsudo su\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\nexit\n\n\nConfiguramos la redirecion del puerto 2222 al 22\nsudo iptables -t nat -I PREROUTING -p tcp -s 172.22.0.0/16 --dport 2222 -j REDIRECT --to-ports 22\n\nsudo iptables -t nat -I PREROUTING -p tcp -s 172.23.0.0/16 --dport 2222 -j REDIRECT --to-ports 22\n\n\nConfiguramos la regla para que se pueda hacer conexión desde el puerto 2222\nsudo iptables -A INPUT -s 172.22.0.0/16 -p tcp -m tcp --dport 2222 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A OUTPUT -d 172.22.0.0/16 -p tcp -m tcp --sport 2222 -m state --state ESTABLISHED -j ACCEPT\n\nsudo iptables -A INPUT -s 172.23.0.0/16 -p tcp -m tcp --dport 2222 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A OUTPUT -d 172.23.0.0/16 -p tcp -m tcp --sport 2222 -m state --state ESTABLISHED -j ACCEPT\n\n\nBloquemos la conexión desde el puerto 22 redirigiendola al Loopback para que se pierda\nsudo iptables -t nat -I PREROUTING -p tcp -s 172.22.0.0/16 --dport 22 --jump DNAT --to-destination 127.0.0.1\n\nsudo iptables -t nat -I PREROUTING -p tcp -s 172.23.0.0/16 --dport 22 --jump DNAT --to-destination 127.0.0.1\n\n\nComprobación\nmoralg@padano:~$ ssh -A -p 2222 debian@172.22.200.145\n    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n    @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n    IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\n    Someone could be eavesdropping on you right now (man-in-the-middle attack)!\n    It is also possible that a host key has just been changed.\n    The fingerprint for the ECDSA key sent by the remote host is\n    SHA256:fnmA6k3OIDwXzXVMgrL3g+JjSjlmzRTU0Ou2xYwDdaE.\n    Please contact your system administrator.\n    Add correct host key in /home/moralg/.ssh/known_hosts to get rid of this message.\n    Offending ECDSA key in /home/moralg/.ssh/known_hosts:40\n      remove with:\n      ssh-keygen -f \"/home/moralg/.ssh/known_hosts\" -R \"[172.22.200.145]:2222\"\n    ECDSA host key for [172.22.200.145]:2222 has changed and you have requested strict  checking.\n    Host key verification failed.\n\nmoralg@padano:~$   ssh-keygen -f \"/home/moralg/.ssh/known_hosts\" -R \"[172.22.200.145]   :2222\"\n    # Host [172.22.200.145]:2222 found: line 40\n    /home/moralg/.ssh/known_hosts updated.\n    Original contents retained as /home/moralg/.ssh/known_hosts.old\n    moralg@padano:~$ ssh -A -p 2222 debian@172.22.200.145\n    Linux router-fw 4.19.0-6-cloud-amd64 #1 SMP Debian 4.19.67-2+deb10u1 (2019-09-20)   x86_64\n\n    The programs included with the Debian GNU/Linux system are free software;\n    the exact distribution terms for each program are described in the\n    individual files in /usr/share/doc/*/copyright.\n\n    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n    permitted by applicable law.\n    Last login: Fri Dec 13 10:02:20 2019 from 172.22.1.248\n\ndebian@router-fw:~$ exit\n\nmoralg@padano:~$ ssh -A -p 22 debian@172.22.200.145\n    ssh: connect to host 172.22.200.145 port 22: Connection timed out\n\n\nTarea 2. Desde la LAN y la DMZ se debe permitir la conexión ssh por el puerto 22 al la máquina router-fw.\n\nReglas\n\nLAN\nsudo iptables -A INPUT -s 192.168.100.10/24 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A OUTPUT -d 192.168.100.10/24 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT\n\n\nDMZ\nsudo iptables -A INPUT -s 192.168.200.10/16 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A OUTPUT -d 192.168.200.10/16 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT\n\n\nComprobación\n\nLAN\ndebian@router-fw:~$ ssh -A debian@192.168.100.10\n    Linux lan 4.19.0-6-cloud-amd64 #1 SMP Debian 4.19.67-2+deb10u1 (2019-09-20) x86_64\n\n    The programs included with the Debian GNU/Linux system are free software;\n    the exact distribution terms for each program are described in the\n    individual files in /usr/share/doc/*/copyright.\n\n    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n    permitted by applicable law.\n    Last login: Fri Dec 13 11:53:58 2019 from 192.168.100.2\n\ndebian@lan:~$ ssh debian@192.168.100.2\n    The authenticity of host '192.168.100.2 (192.168.100.2)' can't be established.\n    ECDSA key fingerprint is SHA256:fnmA6k3OIDwXzXVMgrL3g+JjSjlmzRTU0Ou2xYwDdaE.\n    Are you sure you want to continue connecting (yes/no)? yes\n    Warning: Permanently added '192.168.100.2' (ECDSA) to the list of known hosts.\n    Linux router-fw 4.19.0-6-cloud-amd64 #1 SMP Debian 4.19.67-2+deb10u1 (2019-09-20)   x86_64\n\n    The programs included with the Debian GNU/Linux system are free software;\n    the exact distribution terms for each program are described in the\n    individual files in /usr/share/doc/*/copyright.\n\n    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n    permitted by applicable law.\n    Last login: Fri Dec 13 12:01:34 2019 from 172.22.1.248\n\ndebian@router-fw:~$ exit\n\n\nDMZ\ndebian@router-fw:~$ ssh -A debian@192.168.200.10\n    Linux dmz 4.19.0-6-cloud-amd64 #1 SMP Debian 4.19.67-2+deb10u1 (2019-09-20) x86_64\n\n    The programs included with the Debian GNU/Linux system are free software;\n    the exact distribution terms for each program are described in the\n    individual files in /usr/share/doc/*/copyright.\n\n    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n    permitted by applicable law.\n    Last login: Fri Dec 13 11:23:14 2019 from 192.168.200.2\n\ndebian@dmz:~$ ssh debian@192.168.200.2\n    The authenticity of host '192.168.200.2 (192.168.200.2)' can't be established.\n    ECDSA key fingerprint is SHA256:fnmA6k3OIDwXzXVMgrL3g+JjSjlmzRTU0Ou2xYwDdaE.\n    Are you sure you want to continue connecting (yes/no)? yes\n    Warning: Permanently added '192.168.200.2' (ECDSA) to the list of known hosts.\n    Linux router-fw 4.19.0-6-cloud-amd64 #1 SMP Debian 4.19.67-2+deb10u1 (2019-09-20)   x86_64\n\n    The programs included with the Debian GNU/Linux system are free software;\n    the exact distribution terms for each program are described in the\n    individual files in /usr/share/doc/*/copyright.\n\n    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n    permitted by applicable law.\n    Last login: Fri Dec 13 12:02:10 2019 from 192.168.100.10\n\ndebian@router-fw:~$ exit\n\n\nTarea 3. La máquina router-fw debe tener permitido el tráfico para la interfaz loopback.\n\nReglas\nsudo iptables -A INPUT -i lo -p icmp -j ACCEPT\nsudo iptables -A OUTPUT -o lo -p icmp -j ACCEPT\n\n\nComprobación\ndebian@router-fw:~$ ping 127.0.0.1\n    PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.\n    ping: sendmsg: Operation not permitted\n    ping: sendmsg: Operation not permitted\n    ^Lping: sendmsg: Operation not permitted\n    ping: sendmsg: Operation not permitted\n    ping: sendmsg: Operation not permitted\n    ^C\n    --- 127.0.0.1 ping statistics ---\n    5 packets transmitted, 0 received, 100% packet loss, time 105ms\n\ndebian@router-fw:~$ sudo iptables -A INPUT -i lo -p icmp -j ACCEPT\ndebian@router-fw:~$ sudo iptables -A OUTPUT -o lo -p icmp -j ACCEPT\n\ndebian@router-fw:~$ ping 127.0.0.1\n    PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.\n    64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.050 ms\n    64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.048 ms\n    64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.074 ms\n    64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=0.064 ms\n    ^C\n    --- 127.0.0.1 ping statistics ---\n    4 packets transmitted, 4 received, 0% packet loss, time 77ms\n    rtt min/avg/max/mdev = 0.048/0.059/0.074/0.010 ms\n\n\nTarea 4. A la máquina router-fw se le puede hacer ping desde la DMZ, pero desde la LAN se le debe rechazar la conexión (REJECT).\n\nReglas\n\nLAN\nsudo iptables -A INPUT -i eth1 -s 192.168.100.0/24 -p icmp -m icmp --icmp-type echo-request -j REJECT --reject-with icmp-port-unreachable\nsudo iptables -A OUTPUT -o eth1 -d 192.168.100.0/24 -p icmp -m icmp --icmp-type echo-reply -j ACCEPT\nsudo iptables -A OUTPUT -o eth1 -d 192.168.100.0/24 -p icmp -m state --state RELATED -j ACCEPT\n\n\nDMZ\nsudo iptables -A INPUT -i eth2 -s 192.168.200.0/24 -p icmp -m icmp --icmp-type echo-request -j ACCEPT\nsudo iptables -A OUTPUT -o eth2 -d 192.168.200.0/24 -p icmp -m icmp --icmp-type echo-reply -j ACCEPT\n\n\nComprobación\nLAN\ndebian@lan:~$ ping 192.168.100.2\n    PING 192.168.100.2 (192.168.100.2) 56(84) bytes of data.\n    From 192.168.100.2 icmp_seq=1 Destination Port Unreachable\n    From 192.168.100.2 icmp_seq=2 Destination Port Unreachable\n    From 192.168.100.2 icmp_seq=3 Destination Port Unreachable\n    From 192.168.100.2 icmp_seq=4 Destination Port Unreachable\n    From 192.168.100.2 icmp_seq=5 Destination Port Unreachable\n    ^C\n    --- 192.168.100.2 ping statistics ---\n    5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 16ms\n\n\nDMZ\ndebian@dmz:~$ ping 192.168.200.2\n    PING 192.168.200.2 (192.168.200.2) 56(84) bytes of data.\n    64 bytes from 192.168.200.2: icmp_seq=1 ttl=64 time=0.896 ms\n    64 bytes from 192.168.200.2: icmp_seq=2 ttl=64 time=1.12 ms\n    64 bytes from 192.168.200.2: icmp_seq=3 ttl=64 time=1.08 ms\n    64 bytes from 192.168.200.2: icmp_seq=4 ttl=64 time=1.15 ms\n    64 bytes from 192.168.200.2: icmp_seq=5 ttl=64 time=1.11 ms\n    ^C\n    --- 192.168.200.2 ping statistics ---\n    5 packets transmitted, 5 received, 0% packet loss, time 10ms\n    rtt min/avg/max/mdev = 0.896/1.070/1.145/0.098 ms\n\n\nTarea 5. La máquina router-fw puede hacer ping a la LAN, la DMZ y al exterior.\n\nReglas\n\nLAN\nsudo iptables -A OUTPUT -o eth1 -d 192.168.100.0/24 -p icmp -m icmp --icmp-type echo-request -j ACCEPT\nsudo iptables -A INPUT -i eth1 -s 192.168.100.0/24 -p icmp -m icmp --icmp-type echo-reply -j ACCEPT\n\n\nDMZ\nsudo iptables -A OUTPUT -o eth2 -d 192.168.200.0/24 -p icmp -m icmp --icmp-type echo-request -j ACCEPT\nsudo iptables -A INPUT -i eth2 -s 192.168.200.0/24 -p icmp -m icmp --icmp-type echo-reply -j ACCEPT\n\n\nExterior\nsudo iptables -A OUTPUT -o eth0 -p icmp -m icmp --icmp-type echo-request -j ACCEPT\nsudo iptables -A INPUT -i eth0 -p icmp -m icmp --icmp-type echo-reply -j ACCEPT\n\n\nComprobación\n\nLAN\ndebian@router-fw:~$ ping 192.168.100.10\n    PING 192.168.100.10 (192.168.100.10) 56(84) bytes of data.\n    64 bytes from 192.168.100.10: icmp_seq=1 ttl=64 time=1.02 ms\n    64 bytes from 192.168.100.10: icmp_seq=2 ttl=64 time=0.857 ms\n    64 bytes from 192.168.100.10: icmp_seq=3 ttl=64 time=0.802 ms\n    64 bytes from 192.168.100.10: icmp_seq=4 ttl=64 time=0.696 ms\n    64 bytes from 192.168.100.10: icmp_seq=5 ttl=64 time=0.791 ms\n    ^C\n    --- 192.168.100.10 ping statistics ---\n    5 packets transmitted, 5 received, 0% packet loss, time 10ms\n    rtt min/avg/max/mdev = 0.696/0.832/1.015/0.106 ms\n\n\nDMZ\ndebian@router-fw:~$ ping 192.168.200.10\n    PING 192.168.200.10 (192.168.200.10) 56(84) bytes of data.\n    64 bytes from 192.168.200.10: icmp_seq=1 ttl=64 time=1.80 ms\n    64 bytes from 192.168.200.10: icmp_seq=2 ttl=64 time=0.997 ms\n    64 bytes from 192.168.200.10: icmp_seq=3 ttl=64 time=0.790 ms\n    64 bytes from 192.168.200.10: icmp_seq=4 ttl=64 time=1.05 ms\n    64 bytes from 192.168.200.10: icmp_seq=5 ttl=64 time=1.14 ms\n    ^C\n    --- 192.168.200.10 ping statistics ---\n    5 packets transmitted, 5 received, 0% packet loss, time 10ms\n    rtt min/avg/max/mdev = 0.790/1.154/1.801/0.344 ms\n\n\nExterior\ndebian@router-fw:~$ ping 1.1.1.1\n    PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.\n    64 bytes from 1.1.1.1: icmp_seq=1 ttl=55 time=41.4 ms\n    64 bytes from 1.1.1.1: icmp_seq=2 ttl=55 time=41.0 ms\n    64 bytes from 1.1.1.1: icmp_seq=3 ttl=55 time=41.9 ms\n    64 bytes from 1.1.1.1: icmp_seq=4 ttl=55 time=42.1 ms\n    64 bytes from 1.1.1.1: icmp_seq=5 ttl=55 time=42.10 ms\n    ^C\n    --- 1.1.1.1 ping statistics ---\n    5 packets transmitted, 5 received, 0% packet loss, time 11ms\n    rtt min/avg/max/mdev = 41.032/41.874/42.950/0.673 ms\n\n\nTarea 6. Desde la máquina DMZ se puede hacer ping y conexión ssh a la máquina LAN.\n\nReglas\n\nSSH\nsudo iptables -A FORWARD -i eth2 -o eth1 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth1 -o eth2 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT\n\n\nPING\nsudo iptables -A FORWARD -i eth2 -o eth1 -s 192.168.200.0/24 -p icmp -m icmp --icmp-type echo-request -j ACCEPT\nsudo iptables -A FORWARD -i eth1 -o eth2 -d 192.168.200.0/24 -p icmp -m icmp --icmp-type echo-reply -j ACCEPT\n\n\nComprobación\nSSH\ndebian@dmz:~$ ssh 192.168.100.10\n    The authenticity of host '192.168.100.10 (192.168.100.10)' can't be established.\n    ECDSA key fingerprint is SHA256:YZMnTboVGppp2MCQN/Jz89AI3MR/Rx9pnQrB/R4jmJk.\n    Are you sure you want to continue connecting (yes/no)? yes\n    Warning: Permanently added '192.168.100.10' (ECDSA) to the list of known hosts.\n    Linux lan 4.19.0-6-cloud-amd64 #1 SMP Debian 4.19.67-2+deb10u1 (2019-09-20) x86_64\n\n    The programs included with the Debian GNU/Linux system are free software;\n    the exact distribution terms for each program are described in the\n    individual files in /usr/share/doc/*/copyright.\n\n    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n    permitted by applicable law.\n    Last login: Tue Dec 31 16:44:04 2019 from 192.168.100.2\n\ndebian@lan:~$ exit\n\n\nPING\ndebian@dmz:~$ ping 192.168.100.10\n    PING 192.168.100.10 (192.168.100.10) 56(84) bytes of data.\n    64 bytes from 192.168.100.10: icmp_seq=1 ttl=63 time=1.62 ms\n    64 bytes from 192.168.100.10: icmp_seq=2 ttl=63 time=1.49 ms\n    64 bytes from 192.168.100.10: icmp_seq=3 ttl=63 time=1.87 ms\n    64 bytes from 192.168.100.10: icmp_seq=4 ttl=63 time=1.59 ms\n    64 bytes from 192.168.100.10: icmp_seq=5 ttl=63 time=1.47 ms\n    ^C\n    --- 192.168.100.10 ping statistics ---\n    5 packets transmitted, 5 received, 0% packet loss, time 12ms\n    rtt min/avg/max/mdev = 1.465/1.607/1.867/0.144 ms\n\n\nTarea 7. Desde la máquina LAN no se puede hacer ping, pero si se puede conectar por ssh a la máquina DMZ.\n\nReglas\nSSH\nsudo iptables -A FORWARD -i eth1 -o eth2 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth2 -o eth1 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT\n\n\nPING\n\n  Por defecto esta en DROP\n\n\nComprobación\nSSH\ndebian@lan:~$ ssh 192.168.200.10\n    The authenticity of host '192.168.200.10 (192.168.200.10)' can't be established.\n    ECDSA key fingerprint is SHA256:HDhQ4dDIMfEt0p916tj987SZjuhmhqd+qEjGNLikwN8.\n    Are you sure you want to continue connecting (yes/no)? yes\n    Warning: Permanently added '192.168.200.10' (ECDSA) to the list of known hosts.\n    Linux dmz 4.19.0-6-cloud-amd64 #1 SMP Debian 4.19.67-2+deb10u1 (2019-09-20) x86_64\n\n    The programs included with the Debian GNU/Linux system are free software;\n    the exact distribution terms for each program are described in the\n    individual files in /usr/share/doc/*/copyright.\n\n    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n    permitted by applicable law.\n    Last login: Tue Dec 31 16:46:29 2019 from 192.168.200.2\n\ndebian@dmz:~$ exit\n\n\nPING\ndebian@lan:~$ ping 192.168.200.10\n    PING 192.168.200.10 (192.168.200.10) 56(84) bytes of data.\n    ^C\n    --- 192.168.200.10 ping statistics ---\n    247 packets transmitted, 0 received, 100% packet loss, time 1150ms\n\n\nTarea 8. Configura la máquina router-fw para que las máquinas LAN y DMZ puedan acceder al exterior.\n\nAñadimos las reglas de SNAT para que las máquinas internas puedan acceder al exterior\nsudo iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o eth0 -j MASQUERADE\nsudo iptables -t nat -A POSTROUTING -s 192.168.200.0/24 -o eth0 -j MASQUERADE\n\n\nTarea 9. La máquina LAN se le permite hacer ping al exterior.\n\nReglas\nsudo iptables -A FORWARD -i eth1 -o eth0 -p icmp -m icmp --icmp-type echo-request -j ACCEPT\nsudo iptables -A FORWARD -i eth0 -o eth1 -p icmp -m icmp --icmp-type echo-reply -j ACCEPT\n\n\nComprobación\ndebian@lan:~$ ping 1.1.1.1\n    PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.\n    64 bytes from 1.1.1.1: icmp_seq=1 ttl=54 time=42.6 ms\n    64 bytes from 1.1.1.1: icmp_seq=2 ttl=54 time=41.10 ms\n    64 bytes from 1.1.1.1: icmp_seq=3 ttl=54 time=68.3 ms\n    64 bytes from 1.1.1.1: icmp_seq=4 ttl=54 time=66.2 ms\n    64 bytes from 1.1.1.1: icmp_seq=5 ttl=54 time=43.0 ms\n    ^C\n    --- 1.1.1.1 ping statistics ---\n    5 packets transmitted, 5 received, 0% packet loss, time 11ms\n    rtt min/avg/max/mdev = 41.959/52.429/68.349/12.161 ms\n\n\nTarea 10. La máquina LAN puede navegar.\n\nReglas\nActivamos DNS\nsudo iptables -A FORWARD -i eth1 -o eth0 -p udp --dport 53 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth0 -o eth1 -p udp --sport 53 -m state --state ESTABLISHED -j ACCEPT\n\n\nActivamos HTTP\nsudo iptables -A FORWARD -i eth1 -o eth0 -p tcp -m multiport --dports 80 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth0 -o eth1 -p tcp -m multiport --sports 80 -m state --state ESTABLISHED -j ACCEPT\n\n\nActivamos HTTPS\nsudo iptables -A FORWARD -i eth1 -o eth0 -p tcp -m multiport --dports 443 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth0 -o eth1 -p tcp -m multiport --sports 443 -m state --state ESTABLISHED -j ACCEPT\n\n\nComprobación\nDescargamos un paquete\ndebian@lan:~$ sudo apt install tree\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\nNeed to get 49.3 kB of archives.\nAfter this operation, 117 kB of additional disk space will be used.\nGet:1 http://deb.debian.org/debian buster/main amd64 tree amd64 1.8.0-1 [49.3 kB]\nFetched 49.3 kB in 0s (247 kB/s)\nSelecting previously unselected package tree.\n(Reading database ... 26978 files and directories currently installed.)\nPreparing to unpack .../tree_1.8.0-1_amd64.deb ...\nUnpacking tree (1.8.0-1) ...\nSetting up tree (1.8.0-1) ...\n\n\nTarea 11. La máquina DMZ puede navegar. Instala un servidor web, un servidor ftp y un servidor de correos.\n\nReglas\nActivamos DNS\nsudo iptables -A FORWARD -i eth2 -o eth0 -p udp --dport 53 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth0 -o eth2 -p udp --sport 53 -m state --state ESTABLISHED -j ACCEPT\n\n\nActivamos HTTP\nsudo iptables -A FORWARD -i eth2 -o eth0 -p tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth0 -o eth2 -p tcp --sport 80 -m state --state ESTABLISHED -j ACCEPT\n\n\nActivamos HTTPS\nsudo iptables -A FORWARD -i eth2 -o eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth0 -o eth2 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT\n\n\nComprobación\ndebian@dmz:~$ sudo apt install apache2 postfix proftpd\n    Reading package lists... Done\n    Building dependency tree\n    Reading state information... Done\n    Note, selecting 'proftpd-basic' instead of 'proftpd'\n    The following additional packages will be installed:\n      apache2-bin apache2-data apache2-utils libapr1 libaprutil1 libaprutil1-dbd-sqlite3    libaprutil1-ldap\n      libbrotli1 libcurl4 libgdbm-compat4 libgdbm6 libhiredis0.14 libjansson4   libldap-2.4-2 libldap-common\n      liblua5.2-0 libmemcached11 libmemcachedutil2 libnghttp2-14 libperl5.28 librtmp1   libsasl2-2\n      libsasl2-modules libsasl2-modules-db libssh2-1 perl perl-modules-5.28 proftpd-doc     ssl-cert\n    Suggested packages:\n      apache2-doc apache2-suexec-pristine | apache2-suexec-custom www-browser   libsasl2-modules-gssapi-mit\n      | libsasl2-modules-gssapi-heimdal libsasl2-modules-ldap libsasl2-modules-otp  libsasl2-modules-sql\n      perl-doc libterm-readline-gnu-perl | libterm-readline-perl-perl make libb-debug-perl\n      liblocale-codes-perl procmail postfix-mysql postfix-pgsql postfix-ldap postfix-pcre   postfix-lmdb\n      postfix-sqlite sasl2-bin | dovecot-common resolvconf postfix-cdb mail-reader ufw  postfix-doc\n      openbsd-inetd | inet-superserver proftpd-mod-ldap proftpd-mod-mysql proftpd-mod-odbc\n      proftpd-mod-pgsql proftpd-mod-sqlite proftpd-mod-geoip proftpd-mod-snmp   openssl-blacklist\n    The following NEW packages will be installed:\n      apache2 apache2-bin apache2-data apache2-utils libapr1 libaprutil1    libaprutil1-dbd-sqlite3\n      libaprutil1-ldap libbrotli1 libcurl4 libgdbm-compat4 libgdbm6 libhiredis0.14  libjansson4\n      libldap-2.4-2 libldap-common liblua5.2-0 libmemcached11 libmemcachedutil2     libnghttp2-14 libperl5.28\n      librtmp1 libsasl2-2 libsasl2-modules libsasl2-modules-db libssh2-1 perl   perl-modules-5.28 postfix\n      proftpd-basic proftpd-doc ssl-cert\n    0 upgraded, 32 newly installed, 0 to remove and 0 not upgraded.\n    Need to get 16.9 MB of archives.\n    After this operation, 72.6 MB of additional disk space will be used.\nDo you want to continue? [Y/n] y\n    Get:3 http://deb.debian.org/debian buster/main amd64 perl-modules-5.28 all 5.28.1-6     [2,873 kB]\n\n\nTarea 12. Configura la máquina router-fw para que los servicios web y ftp sean accesibles desde el exterior.\n\n\n  Próximamente.\n\n\nTarea 13. El servidor web y el servidor ftp deben ser accesible desde la LAN y desde el exterior.\n\n\n  Próximamente.\n\n\nTarea 14. El servidor de correos sólo debe ser accesible desde la LAN.\n\nEn la instalación elegimos internet site.\n\n   ┌───────────────────────────────────┤ Postfix Configuration ├────────────────────────────────────┐\n   │ Escoja el tipo de configuración del servidor de correo que se ajusta mejor a sus necesidades.  │\n   │                                                                                                │\n   │  Sin configuración:                                                                            │\n   │   Mantiene la configuración actual intacta.                                                    │\n   │  Sitio de Internet:                                                                            │\n   │   El correo se envía y recibe directamente utilizando SMTP.                                    │\n   │  Internet con «smarthost»:                                                                     │\n   │   El correo se recibe directamente utilizando SMTP o ejecutando una                            │\n   │   herramienta como «fetchmail». El correo de salida se envía utilizando                        │\n   │   un «smarthost».                                                                              │\n   │  Sólo correo local:                                                                            │\n   │   El único correo que se entrega es para los usuarios locales. No                              │\n   │   hay red.                                                                                     │\n   │                                                                                                │\n   │ Tipo genérico de configuración de correo:                                                      │\n   │                                                                                                │\n   │                                   Sin configuración                                            │\n   │                            &gt;&gt;&gt;&gt;   Sitio de Internet                                            │\n   │                                   Internet con «smarthost»                                     │\n   │                                   Sistema satélite                                             │\n   │                                   Sólo correo local                                            │\n   │                                                                                                │\n   │                                                                                                │\n   │                           &lt;Aceptar&gt;                          &lt;Cancelar&gt;                        │\n   │                                                                                                │\n   └────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\nAhora vamos a especificar las redes permitidas por el servidor de correos en el fichero /etc/postfix/main.cf, modificando las siguiente linea:\n\nmynetworks = 127.0.0.0/8 192.168.100.0/24\n\n\nReglas\nsudo iptables -A FORWARD -i eth1 -o eth2 -p tcp --dport 25 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth2 -o eth1 -p tcp --sport 25 -m state --state ESTABLISHED -j ACCEPT\n\n\nComprobación\ndebian@lan:~$ telnet 192.168.200.10 25\n    Trying 192.168.200.10...\n    Connected to 192.168.200.10.\n    Escape character is '^]'.\n    220 dmz.novalocal ESMTP Postfix (Debian/GNU)\n\n\nTarea 15. En la máquina LAN instala un servidor mysql. A este servidor sólo se puede acceder desde la DMZ.\n\nInstalamos en la lan el servidor y en la dmz el cliente de mariadb.\n\nEn la la LAN\nsudo apt install mariadb-server\n\n\nEn la DMZ\n\nsudo apt install mariadb-client\n\n\nCreamos en el servidor la una base de datos y un usuario, además le asignamos los permisos para que podamos acceder desde la DMZ\n\nMariaDB [(none)]&gt; CREATE DATABASE prueba;\n    Query OK, 1 row affected (0.001 sec)\n\nMariaDB [(none)]&gt; CREATE USER user_prueba@\"192.168.200.10\" IDENTIFIED BY \"user_prueba\";\n    Query OK, 0 rows affected (0.045 sec)\n\nMariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON prueba.* to user_prueba IDENTIFIED BY \"user_prueba\";\n    Query OK, 0 rows affected (0.001 sec)\n\nMariaDB [(none)]&gt; FLUSH PRIVILEGES;\n    Query OK, 0 rows affected (0.001 sec)\n\n\nEditamos el fichero /etc/mysql/mariadb.conf.d/50-server.cnf y añadimos la siguiente linea:\n\nbind-address            = 0.0.0.0\n\n\nReiniciamos el sericio:\n\nsudo systemctl restart mariadb.service \n\n\nReglas\nsudo iptables -A FORWARD -i eth2 -o eth1 -p tcp --dport 3306 -m state --state NEW,ESTABLISHED -j ACCEPT\nsudo iptables -A FORWARD -i eth1 -o eth2 -p tcp --sport 3306 -m state --state ESTABLISHED -j ACCEPT\n\n\nComprobación\ndebian@dmz:~$ sudo mysql -u user_prueba -p prueba -h 192.168.100.10\nEnter password: \n    Welcome to the MariaDB monitor.  Commands end with ; or \\g.\n    Your MariaDB connection id is 36\n    Server version: 10.3.18-MariaDB-0+deb10u1 Debian 10\n\n    Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\n    Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [prueba]&gt; use prueba\n    Database changed\nMariaDB [prueba]&gt;\n\n\nMejoras\n\n\nMEJORA 1: Implementar que el cortafuego funcione después de un reinicio de la máquina.\n\nCon root@router-fw: creamos el fichero iptableSave.v4 donde guardaremos todas la reglas de iptables que esten activas en el momento de ejecitar dicho comando.\niptables-save &gt; /etc/iproute2/iptableSave.v4\n\n\nroot@router-fw:~# cat /etc/iproute2/iptableSave.v4 \n    # Generated by xtables-save v1.8.2 on Fri Jan  3 18:03:13 2020\n    *filter\n    :INPUT DROP [29:4732]\n    :FORWARD DROP [0:0]\n    :OUTPUT DROP [636:48678]\n    -A INPUT -s 172.22.0.0/16 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED     -j ACCEPT\n    -A INPUT -s 172.23.0.0/16 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED     -j ACCEPT\n    -A INPUT -s 192.168.100.0/24 -i eth1 -p tcp -m tcp --sport 22 -m state --state  ESTABLISHED -j ACCEPT\n    -A INPUT -s 192.168.200.0/24 -i eth2 -p tcp -m tcp --sport 22 -m state --state  ESTABLISHED -j ACCEPT\n    -A INPUT -s 172.22.0.0/16 -p tcp -m tcp --dport 2222 -m state --state NEW,ESTABLISHED   -j ACCEPT\n    -A INPUT -s 172.23.0.0/16 -p tcp -m tcp --dport 2222 -m state --state NEW,ESTABLISHED   -j ACCEPT\n    -A INPUT -s 192.168.100.0/24 -p tcp -m tcp --dport 22 -m state --state NEW, ESTABLISHED -j ACCEPT\n    -A INPUT -s 192.168.0.0/16 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED    -j ACCEPT\n    -A INPUT -i lo -p icmp -j ACCEPT\n    -A INPUT -s 192.168.100.0/24 -i eth1 -p icmp -m icmp --icmp-type 8 -j REJECT    --reject-with icmp-port-unreachable\n    -A INPUT -s 192.168.200.0/24 -i eth2 -p icmp -m icmp --icmp-type 8 -j ACCEPT\n    -A INPUT -s 192.168.100.0/24 -i eth1 -p icmp -m icmp --icmp-type 0 -j ACCEPT\n    -A INPUT -s 192.168.200.0/24 -i eth2 -p icmp -m icmp --icmp-type 0 -j ACCEPT\n    -A INPUT -i eth0 -p icmp -m icmp --icmp-type 0 -j ACCEPT\n    -A FORWARD -i eth2 -o eth1 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED    -j ACCEPT\n    -A FORWARD -i eth1 -o eth2 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j     ACCEPT\n    -A FORWARD -s 192.168.200.0/24 -i eth2 -o eth1 -p icmp -m icmp --icmp-type 8 -j ACCEPT\n    -A FORWARD -d 192.168.200.0/24 -i eth1 -o eth2 -p icmp -m icmp --icmp-type 0 -j ACCEPT\n    -A FORWARD -i eth1 -o eth2 -p tcp -m tcp --dport 22 -m state --state NEW,ESTABLISHED    -j ACCEPT\n    -A FORWARD -i eth2 -o eth1 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j     ACCEPT\n    -A FORWARD -i eth1 -o eth0 -p icmp -m icmp --icmp-type 8 -j ACCEPT\n    -A FORWARD -i eth0 -o eth1 -p icmp -m icmp --icmp-type 0 -j ACCEPT\n    -A FORWARD -i eth1 -o eth0 -p udp -m udp --dport 53 -m state --state NEW,ESTABLISHED    -j ACCEPT\n    -A FORWARD -i eth0 -o eth1 -p udp -m udp --sport 53 -m state --state ESTABLISHED -j     ACCEPT\n    -A FORWARD -i eth1 -o eth0 -p tcp -m multiport --dports 80 -m state --state NEW,    ESTABLISHED -j ACCEPT\n    -A FORWARD -i eth0 -o eth1 -p tcp -m multiport --sports 80 -m state --state     ESTABLISHED -j ACCEPT\n    -A FORWARD -i eth1 -o eth0 -p tcp -m multiport --dports 443 -m state --state NEW,   ESTABLISHED -j ACCEPT\n    -A FORWARD -i eth0 -o eth1 -p tcp -m multiport --sports 443 -m state --state    ESTABLISHED -j ACCEPT\n    -A FORWARD -i eth2 -o eth0 -p udp -m udp --dport 53 -m state --state NEW,ESTABLISHED    -j ACCEPT\n    -A FORWARD -i eth0 -o eth2 -p udp -m udp --sport 53 -m state --state ESTABLISHED -j     ACCEPT\n    -A FORWARD -i eth2 -o eth0 -p tcp -m tcp --dport 80 -m state --state NEW,ESTABLISHED    -j ACCEPT\n    -A FORWARD -i eth0 -o eth2 -p tcp -m tcp --sport 80 -m state --state ESTABLISHED -j     ACCEPT\n    -A FORWARD -i eth2 -o eth0 -p tcp -m tcp --dport 443 -m state --state NEW,ESTABLISHED   -j ACCEPT\n    -A FORWARD -i eth0 -o eth2 -p tcp -m tcp --sport 443 -m state --state ESTABLISHED -j    ACCEPT\n    -A FORWARD -i eth1 -o eth2 -p tcp -m tcp --dport 25 -m state --state NEW,ESTABLISHED    -j ACCEPT\n    -A FORWARD -i eth2 -o eth1 -p tcp -m tcp --sport 25 -m state --state ESTABLISHED -j     ACCEPT\n    -A FORWARD -i eth2 -o eth1 -p tcp -m tcp --dport 3306 -m state --state NEW, ESTABLISHED -j ACCEPT\n    -A FORWARD -i eth1 -o eth2 -p tcp -m tcp --sport 3306 -m state --state ESTABLISHED -j   ACCEPT\n    -A OUTPUT -d 172.22.0.0/16 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j     ACCEPT\n    -A OUTPUT -d 172.23.0.0/16 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j     ACCEPT\n    -A OUTPUT -d 192.168.100.0/24 -o eth1 -p tcp -m tcp --dport 22 -m state --state NEW,    ESTABLISHED -j ACCEPT\n    -A OUTPUT -d 192.168.200.0/24 -o eth2 -p tcp -m tcp --dport 22 -m state --state NEW,    ESTABLISHED -j ACCEPT\n    -A OUTPUT -d 172.22.0.0/16 -p tcp -m tcp --sport 2222 -m state --state ESTABLISHED -j   ACCEPT\n    -A OUTPUT -d 172.23.0.0/16 -p tcp -m tcp --sport 2222 -m state --state ESTABLISHED -j   ACCEPT\n    -A OUTPUT -d 192.168.100.0/24 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED     -j ACCEPT\n    -A OUTPUT -d 192.168.0.0/16 -p tcp -m tcp --sport 22 -m state --state ESTABLISHED -j    ACCEPT\n    -A OUTPUT -o lo -p icmp -j ACCEPT\n    -A OUTPUT -d 192.168.100.0/24 -o eth1 -p icmp -m icmp --icmp-type 0 -j ACCEPT\n    -A OUTPUT -d 192.168.100.0/24 -o eth1 -p icmp -m state --state RELATED -j ACCEPT\n    -A OUTPUT -d 192.168.200.0/24 -o eth2 -p icmp -m icmp --icmp-type 0 -j ACCEPT\n    -A OUTPUT -d 192.168.100.0/24 -o eth1 -p icmp -m icmp --icmp-type 8 -j ACCEPT\n    -A OUTPUT -d 192.168.200.0/24 -o eth2 -p icmp -m icmp --icmp-type 8 -j ACCEPT\n    -A OUTPUT -o eth0 -p icmp -m icmp --icmp-type 8 -j ACCEPT\n    COMMIT\n    # Completed on Fri Jan  3 18:03:13 2020\n    # Generated by xtables-save v1.8.2 on Fri Jan  3 18:03:13 2020\n    *nat\n    :PREROUTING ACCEPT [13:875]\n    :INPUT ACCEPT [3:180]\n    :POSTROUTING ACCEPT [6:580]\n    :OUTPUT ACCEPT [625:47026]\n    -A PREROUTING -s 172.23.0.0/16 -p tcp -m tcp --dport 22 -j DNAT --to-destination    127.0.0.1\n    -A PREROUTING -s 172.22.0.0/16 -p tcp -m tcp --dport 22 -j DNAT --to-destination    127.0.0.1\n    -A PREROUTING -s 172.23.0.0/16 -p tcp -m tcp --dport 2222 -j REDIRECT --to-ports 22\n    -A PREROUTING -s 172.22.0.0/16 -p tcp -m tcp --dport 2222 -j REDIRECT --to-ports 22\n    -A POSTROUTING -s 192.168.100.0/24 -o eth0 -j MASQUERADE\n    -A POSTROUTING -s 192.168.200.0/24 -o eth0 -j MASQUERADE\n    COMMIT\n    # Completed on Fri Jan  3 18:03:13 2020\n\n\nCreamos la unidad de systemd, para esto, tenemos que crear el fichero /etc/systemd/system/restart-iptables.service y añadir las siguientes lineas:\n\n[Unit]\nDescription=restaurar las iptables\nAfter=networking.service\n\n[Service]\nExecStart=/usr/local/bin/restart-iptables.sh\n\n[Install]\nWantedBy=multi-user.target\n\n\nCreamos el script en la ruta indicada en el fichero de systemd /usr/local/bin/restart-iptables.sh\n\n#!/bin/bash\niptables-restore &lt; /etc/iproute2/iptableSave.v4\necho \"Reglas de iptables restauradas\"\n\n\nCambiamos los permisos del script:\n\nchmod 744 /usr/local/bin/restart-iptables.sh\n\n\nActivamos el servicio para que siempre se active al inicio del sistema:\n\nsystemctl enable restart-iptables.service\n    Created symlink /etc/systemd/system/multi-user.target.wants/restart-iptables.service → /etc/systemd/system/restart-iptables.service.\n\n\nIniciamos el servicio:\n\nsystemctl start restart-iptables.service\n\n\nComprobación:\n\nroot@router-fw:~# reboot\nroot@router-fw:~# Connection to 172.22.200.145 closed by remote host.\n    Connection to 172.22.200.145 closed.\n\nmoralg@padano:~$ ssh -A -p 2222 debian@172.22.200.145\n    Linux router-fw 4.19.0-6-cloud-amd64 #1 SMP Debian 4.19.67-2+deb10u1 (2019-09-20)   x86_64\n\n    The programs included with the Debian GNU/Linux system are free software;\n    the exact distribution terms for each program are described in the\n    individual files in /usr/share/doc/*/copyright.\n\n    Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n    permitted by applicable law.\n    Last login: Fri Jan  3 17:03:31 2020 from 172.23.0.54\n\ndebian@router-fw:~$ sudo iptables -n -L\n    Chain INPUT (policy DROP)\n    target     prot opt source               destination         \n    ACCEPT     tcp  --  172.22.0.0/16        0.0.0.0/0            tcp dpt:22 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  172.23.0.0/16        0.0.0.0/0            tcp dpt:22 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  192.168.100.0/24     0.0.0.0/0            tcp spt:22 state  ESTABLISHED\n    ACCEPT     tcp  --  192.168.200.0/24     0.0.0.0/0            tcp spt:22 state  ESTABLISHED\n    ACCEPT     tcp  --  172.22.0.0/16        0.0.0.0/0            tcp dpt:2222 state NEW,   ESTABLISHED\n    ACCEPT     tcp  --  172.23.0.0/16        0.0.0.0/0            tcp dpt:2222 state NEW,   ESTABLISHED\n    ACCEPT     tcp  --  192.168.100.0/24     0.0.0.0/0            tcp dpt:22 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  192.168.0.0/16       0.0.0.0/0            tcp dpt:22 state NEW, ESTABLISHED\n    ACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0           \n    REJECT     icmp --  192.168.100.0/24     0.0.0.0/0            icmptype 8 reject-with    icmp-port-unreachable\n    ACCEPT     icmp --  192.168.200.0/24     0.0.0.0/0            icmptype 8\n    ACCEPT     icmp --  192.168.100.0/24     0.0.0.0/0            icmptype 0\n    ACCEPT     icmp --  192.168.200.0/24     0.0.0.0/0            icmptype 0\n    ACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0            icmptype 0\n\n    Chain FORWARD (policy DROP)\n    target     prot opt source               destination         \n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:22 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp spt:22 state  ESTABLISHED\n    ACCEPT     icmp --  192.168.200.0/24     0.0.0.0/0            icmptype 8\n    ACCEPT     icmp --  0.0.0.0/0            192.168.200.0/24     icmptype 0\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:22 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp spt:22 state  ESTABLISHED\n    ACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0            icmptype 8\n    ACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0            icmptype 0\n    ACCEPT     udp  --  0.0.0.0/0            0.0.0.0/0            udp dpt:53 state NEW, ESTABLISHED\n    ACCEPT     udp  --  0.0.0.0/0            0.0.0.0/0            udp spt:53 state  ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            multiport dports 80   state NEW,ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            multiport sports 80   state ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            multiport dports 443  state NEW,ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            multiport sports 443  state ESTABLISHED\n    ACCEPT     udp  --  0.0.0.0/0            0.0.0.0/0            udp dpt:53 state NEW, ESTABLISHED\n    ACCEPT     udp  --  0.0.0.0/0            0.0.0.0/0            udp spt:53 state  ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:80 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp spt:80 state  ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:443 state NEW,    ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp spt:443 state     ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:25 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp spt:25 state  ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:3306 state NEW,   ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            tcp spt:3306 state    ESTABLISHED\n\n    Chain OUTPUT (policy DROP)\n    target     prot opt source               destination         \n    ACCEPT     tcp  --  0.0.0.0/0            172.22.0.0/16        tcp spt:22 state  ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            172.23.0.0/16        tcp spt:22 state  ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            192.168.100.0/24     tcp dpt:22 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            192.168.200.0/24     tcp dpt:22 state NEW, ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            172.22.0.0/16        tcp spt:2222 state    ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            172.23.0.0/16        tcp spt:2222 state    ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            192.168.100.0/24     tcp spt:22 state  ESTABLISHED\n    ACCEPT     tcp  --  0.0.0.0/0            192.168.0.0/16       tcp spt:22 state  ESTABLISHED\n    ACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0           \n    ACCEPT     icmp --  0.0.0.0/0            192.168.100.0/24     icmptype 0\n    ACCEPT     icmp --  0.0.0.0/0            192.168.100.0/24     state RELATED\n    ACCEPT     icmp --  0.0.0.0/0            192.168.200.0/24     icmptype 0\n    ACCEPT     icmp --  0.0.0.0/0            192.168.100.0/24     icmptype 8\n    ACCEPT     icmp --  0.0.0.0/0            192.168.200.0/24     icmptype 8\n    ACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0            icmptype 8\n\n\nMEJORA 2: Utiliza nuevas cadenas para clasificar el tráfico.\n\n\n  Próximamente.\n\n\nMEJORA 3: Consruye el cortafuego utilizando nftables.\n\nSustituyendo en cada regla iptables por iptables-translate nos saldrá por pantalla la regla traduccida a nftables, pero con algunos errores.\n\ndebian@router-fw:~$ sudo iptables-translate -A FORWARD -i eth1 -o eth0 -p tcp -m multiport --dports 443 -m state --state NEW,ESTABLISHED -j ACCEPT\n    nft add rule ip filter FORWARD iifname \"eth1\" oifname \"eth0\" ip protocol tcp tcp dport 443 ct state new,established  counter accept\n\n\nComo vemos en la regla de nftables que nos devuelve, encontramos los siguientes fallos:\n\n\n  ip: En nftables sería inet\n  FORWARD: En nftables tendría que ser forward en minúsculas\n\n\nPara solucionar estos fallos creamos el siguiente SCRIPT:\n\n#!/bin/sh\n\nsudo rm /home/debian/fichero1 2&gt; /dev/null\nsudo rm /home/debian/fichero2 2&gt; /dev/null\nsudo rm /home/debian/fichero3 2&gt; /dev/null\n\nsudo iptables -S &gt; /home/debian/fichero1\nsudo iptables -t nat -S &gt;&gt; /home/debian/fichero1\n\nwhile read linea \ndo\n    sudo iptables-translate $linea &gt;&gt; /home/debian/fichero2\ndone &lt; fichero1\n\nsed 's/ ip /inet/g' /home/debian/fichero2 &gt; /home/debian/fichero3\ntr A-Z a-z &lt; /home/debian/fichero3 &gt; /home/debian/nftables.txt \n\nsudo rm /home/debian/fichero1 2&gt; /dev/null\nsudo rm /home/debian/fichero2 2&gt; /dev/null\nsudo rm /home/debian/fichero3 2&gt; /dev/null\n\n\nEjecutamos el script:\nbash Translate-iptables.sh \n\n\nNos crea un fichero con la reglas de nftables corregidas.\ncat nftables.txt \n    nft add rule inet filter input inet saddr 172.22.0.0/16 tcp dport 22 ct state new,established  counter accept\n    nft add rule inet filter input inet saddr 172.23.0.0/16 tcp dport 22 ct state new,established  counter accept\n    nft add rule inet filter input iifname \"eth1\" inet saddr 192.168.100.0/24 tcp sport 22 ct state established  counter accept\n    nft add rule inet filter input iifname \"eth2\" inet saddr 192.168.200.0/24 tcp sport 22 ct state established  counter accept\n    nft add rule inet filter input inet saddr 172.22.0.0/16 tcp dport 2222 ct state new,established  counter accept\n    nft add rule inet filter input inet saddr 172.23.0.0/16 tcp dport 2222 ct state new,established  counter accept\n    nft add rule inet filter input inet saddr 192.168.100.0/24 tcp dport 22 ct state new,established  counter accept\n    nft add rule inet filter input inet saddr 192.168.0.0/16 tcp dport 22 ct state new,established  counter accept\n    nft add rule inet filter input iifname \"lo\" inet protocol icmp counter accept\n    nft add rule inet filter input iifname \"eth1\" inet saddr 192.168.100.0/24 icmp type echo-request counter reject\n    nft add rule inet filter input iifname \"eth2\" inet saddr 192.168.200.0/24 icmp type echo-request counter accept\n    nft add rule inet filter input iifname \"eth1\" inet saddr 192.168.100.0/24 icmp type echo-reply counter accept\n    nft add rule inet filter input iifname \"eth2\" inet saddr 192.168.200.0/24 icmp type echo-reply counter accept\n    nft add rule inet filter input iifname \"eth0\" icmp type echo-reply counter accept\n    nft add rule inet filter forward iifname \"eth2\" oifname \"eth1\" tcp dport 22 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth2\" tcp sport 22 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth2\" oifname \"eth1\" inet saddr 192.168.200.0/24 icmp type echo-request counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth2\" inet daddr 192.168.200.0/24 icmp type echo-reply counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth2\" tcp dport 22 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth2\" oifname \"eth1\" tcp sport 22 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth0\" icmp type echo-request counter accept\n    nft add rule inet filter forward iifname \"eth0\" oifname \"eth1\" icmp type echo-reply counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth0\" udp dport 53 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth0\" oifname \"eth1\" udp sport 53 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth0\" inet protocol tcp tcp dport 80 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth0\" oifname \"eth1\" inet protocol tcp tcp sport 80 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth0\" inet protocol tcp tcp dport 443 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth0\" oifname \"eth1\" inet protocol tcp tcp sport 443 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth2\" oifname \"eth0\" udp dport 53 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth0\" oifname \"eth2\" udp sport 53 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth2\" oifname \"eth0\" tcp dport 80 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth0\" oifname \"eth2\" tcp sport 80 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth2\" oifname \"eth0\" tcp dport 443 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth0\" oifname \"eth2\" tcp sport 443 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth2\" tcp dport 25 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth2\" oifname \"eth1\" tcp sport 25 ct state established  counter accept\n    nft add rule inet filter forward iifname \"eth2\" oifname \"eth1\" tcp dport 3306 ct state new,established  counter accept\n    nft add rule inet filter forward iifname \"eth1\" oifname \"eth2\" tcp sport 3306 ct state established  counter accept\n    nft add rule inet filter output inet daddr 172.22.0.0/16 tcp sport 22 ct state established  counter accept\n    nft add rule inet filter output inet daddr 172.23.0.0/16 tcp sport 22 ct state established  counter accept\n    nft add rule inet filter output oifname \"eth1\" inet daddr 192.168.100.0/24 tcp dport 22 ct state new,established  counter accept\n    nft add rule inet filter output oifname \"eth2\" inet daddr 192.168.200.0/24 tcp dport 22 ct state new,established  counter accept\n    nft add rule inet filter output inet daddr 172.22.0.0/16 tcp sport 2222 ct state established  counter accept\n    nft add rule inet filter output inet daddr 172.23.0.0/16 tcp sport 2222 ct state established  counter accept\n    nft add rule inet filter output inet daddr 192.168.100.0/24 tcp sport 22 ct state established  counter accept\n    nft add rule inet filter output inet daddr 192.168.0.0/16 tcp sport 22 ct state established  counter accept\n    nft add rule inet filter output oifname \"lo\" inet protocol icmp counter accept\n    nft add rule inet filter output oifname \"eth1\" inet daddr 192.168.100.0/24 icmp type echo-reply counter accept\n    nft add rule inet filter output oifname \"eth1\" inet protocol icmp inet daddr 192.168.100.0/24 ct state related  counter accept\n    nft add rule inet filter output oifname \"eth2\" inet daddr 192.168.200.0/24 icmp type echo-reply counter accept\n    nft add rule inet filter output oifname \"eth1\" inet daddr 192.168.100.0/24 icmp type echo-request counter accept\n    nft add rule inet filter output oifname \"eth2\" inet daddr 192.168.200.0/24 icmp type echo-request counter accept\n    nft add rule inet filter output oifname \"eth0\" icmp type echo-request counter accept\n    nft add rule inet filter prerouting inet saddr 172.23.0.0/16 tcp dport 22 counter dnat to 127.0.0.1\n    nft add rule inet filter prerouting inet saddr 172.22.0.0/16 tcp dport 22 counter dnat to 127.0.0.1\n    nft add rule inet filter prerouting inet saddr 172.23.0.0/16 tcp dport 2222 counter redirect to :22\n    nft add rule inet filter prerouting inet saddr 172.22.0.0/16 tcp dport 2222 counter redirect to :22\n    nft add rule inet filter postrouting oifname \"eth0\" inet saddr 192.168.100.0/24 counter masquerade \n    nft add rule inet filter postrouting oifname \"eth0\" inet saddr 192.168.200.0/24 counter masquerade \n\n",
        "url": "/seguridad/2020/01/12/Cortafuego_Perimetral_con_DMZ/"
      },
    
      {
        "title": "Trabajando con iSCSI",
        "excerpt": "Vamos a configurar un sistema que exporte algunos targets por iSCSI y los conecte a diversos clientes.\n",
        "content": "\n\nVamos a configurar un sistema que exporte algunos targets por iSCSI y los conecte a diversos clientes, explicando con detalle la forma de trabajar.\n\nVamos a crear un escenario con Vagrant. Vamos a crear una máquina servidor, conectada por una red interna a dos cliente, un cliente Debian y otro Window. al servidor le añadiremos 3 volúmenes. Añadiremos un interfaz con conexión a internet para poder descargar los paquetes necesarios.\n\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define :servidor1 do |servidor1|\n    d1 = \".vagrant/disco1.vdi\"\n    d2 = \".vagrant/disco2.vdi\"\n    d3 = \".vagrant/disco3.vdi\"\n    servidor1.vm.box = \"debian/buster64\"\n    servidor1.vm.hostname = \"Servidor1\"\n\n    servidor1.vm.network :public_network,:bridge=&gt;\"enp3s0\"\n    #servidor.vm.network :public_network,:bridge=&gt;\"wlp2s0\"\n    servidor1.vm.network :private_network, ip: \"192.168.100.1\", virtualbox__intnet:\"mired1\"\n\n    servidor1.vm.provider :virtualbox do |v|\n\t    if not File.exist?(d1)\n\t    \tv.customize [\"createhd\", \"--filename\", d1, \"--size\", 1 *  1024]\n\t    end\n\t    v.customize [\"storageattach\", :id, \"--storagectl\", \"SATA Controller\", \"--port\", 1,\"--device\", 0, \"--type\", \"hdd\", \"--medium\", d1]\n\n\t    if not File.exist?(d2)\n\t    \tv.customize [\"createhd\", \"--filename\", d2, \"--size\", 1 * 1024]\n\t    end\n\t    v.customize [\"storageattach\", :id, \"--storagectl\", \"SATA Controller\", \"--port\", 2,\"--device\", 0, \"--type\", \"hdd\", \"--medium\", d2]\n\n        if not File.exist?(d3)\n            v.customize [\"createhd\", \"--filename\", d3, \"--size\", 1 * 1024]\n        end\n        v.customize [\"storageattach\", :id, \"--storagectl\", \"SATA Controller\", \"--port\", 3,\"--device\", 0, \"--type\", \"hdd\", \"--medium\", d3]\n    end\n  end\n\n  config.vm.define :cliente1 do |cliente1|\n    cliente1.vm.box = \"debian/buster64\"\n    cliente1.vm.hostname = \"Cliente1\"\n    cliente1.vm.network :public_network,:bridge=&gt;\"enp3s0\"\n    #cliente1.vm.network :public_network,:bridge=&gt;\"wlp2s0\"\n    cliente1.vm.network :private_network, ip: \"192.168.100.2\", virtualbox__intnet:\"mired1\"\n  end\n\n  config.vm.define :cliente1 do |cliente2|\n    cliente2.vm.box = \"vdelarosa/windows-10\"\n    cliente2.vm.hostname = \"Cliente2\"\n    cliente2.vm.network :public_network,:bridge=&gt;\"enp3s0\"\n    #cliente2.vm.network :public_network,:bridge=&gt;\"wlp2s0\"\n    cliente2.vm.network :private_network, ip: \"192.168.100.3\", virtualbox__intnet:\"mired1\"\n  end\n\nend\n\n\nTarea 1\n\nCrearemos un target con una LUN y lo conectaremos a un cliente GNU/Linux. ¿Cómo escaneamos desde el cliente buscando los targets disponibles y utilizando la unidad lógica proporcionada?, formateándola si es necesario y montándola.\n\nConceptos:\n\n\n  iSCSI: Es un estándar de red de almacenamiento que vincula instalaciones de almacenamiento de datos. Facilita la transferencia de datos a través de redes LAN, WAN o Internet. Todo esto quiere decir que el espacio en el servidor de almacenamiento será considerado como discos locales por el sistema operativo del cliente pero todos los datos tranferidos al disco se transfiren realmente a través de la red al servidor de almacenamiento.\n\n  LUN (Logical Unit Number): Un LUN se representa como un dispositivo SCSI direccionable individualmente que forma parte de un dispositivo SCDI físico, es decir, una dirección que identifica un disco completo o un trocito de un dispositivo de bloque del Servidor.\n\n  Target: Es el encargado de asignar virtualmente los LUN creados al sistema operativo del cliente.\n\n\nConfiguración del servidor\n\nEn el servidor hemos asignado tres discos de 1GB, los cuales vamos a agrupar en un LVM para tener un espacio de almacenamiento de 3GB y poder gestionarlo como si fuese un solo dispositivo de bloque.\n\nComprobamos los disco asignados al Servidor\nlsblk -l\n    NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n    sda    8:0    0 19.8G  0 disk \n    sda1   8:1    0 18.8G  0 part /\n    sda2   8:2    0    1K  0 part \n    sda5   8:5    0 1021M  0 part [SWAP]\n    sdb    8:16   0    1G  0 disk \n    sdc    8:32   0    1G  0 disk \n    sdd    8:48   0    1G  0 disk \n\n\nVamos a descargar el paquete lvm2 que instalará los paquetes del kernel, el daemon del espacio de usuario y todo lo demás que necesitamos para trabajar con LVM.\n\nDescargamos e instalamos lvm2\nsudo apt install lvm2\n\n\nAhora vamos a definir los volúmenes físicos donde indicaremos el disco sdb.\n\nDefiniendo el volumen físico\nsudo pvcreate /dev/sdb\n  Physical volume \"/dev/sdb\" successfully created.\n\n\nDespués de definir el volumen físico tenemos que crear el grupo de volúmenes. a este grupo de volúmenes lo llamaremo vgCliente1 con el volumen físico sdb.\n\nCreando el grupo de volúmenes\nsudo vgcreate vgCliente1 /dev/sdb\n  Volume group \"vgCliente1\" successfully created\n\n\nAhora vamos a crear un volumen lógico, el cual va a ser el espacio que contendrá el sistema de ficheros. Este lo podemos crear del tamaño que queramos dependiendo del tamaño todal del grupo de volúmenes, en nuestro caso va a ser de 500Mb y lo llamaremos lv1 perteneciente al grupo de volumenes vgCliente1.\n\nCreando el volumen lógico\nsudo lvcreate -L 500M -n lv1 vgCliente1\n  Logical volume \"lv1\" created.\n\n\nYa tenemos nuestro volumen lógico creado, ahora podemos ya trabajar con el para crear el LUN y asignarlo mediante un target al cliente.\n\nComprobamos que se ha creado de forma correcta\nsudo lvs\n  LV   VG         Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n  lv1  vgCliente1 -wi-a----- 500.00m\n\nsudo lvdisplay \n  --- Logical volume ---\n  LV Path                /dev/vgCliente1/lv1\n  LV Name                lv1\n  VG Name                vgCliente1\n  LV UUID                xR20wc-5YFN-af3g-UQGD-LH6Y-MoUL-3F7H2G\n  LV Write Access        read/write\n  LV Creation host, time Servidor1, 2020-02-01 12:30:02 +0000\n  LV Status              available\n  # open                 0\n  LV Size                500.00 MiB\n  Current LE             125\n  Segments               1\n  Allocation             inherit\n  Read ahead sectors     auto\n  - currently set to     256\n  Block device           254:0\n\n\nYa tenemos todo lo necesario para trabajar con iSCSI pero tenemos que descargar el paquete tgt, que va a instalar los paquetes necesario para crear target.\n\nDescargamos e instalamos el paquete tgt\nsudo apt install tgt\n\n\nAhora tenemmos que crear el target, que se configura el fichero /etc/tgt/targets.conf con la siguiente configuración:\n\n\n\n&lt;target &lt;Estructura _del_target&gt;&gt;\n    backing-store &lt;Nombre_del_volumen_lógico&gt;\n&lt;/target&gt;\n\n\n  \n    Estructura de los target iSCSI: iqn.&lt;AÑO&gt;-&lt;MES&gt;.&lt;DOMINO_INVERTIDO&gt;:&lt;NOMBRE_DE_LA_MÁQUINA&gt;.&lt;TARGET&gt;&lt;NÚMERO&gt;\n  \n  \n    Nombre del volumen logico:\n    sudo lvdisplay | egrep \"LV Path\" | awk '{ print $3 }'\n/dev/vgCliente1/lv1\n    \n    \n  \n\n\nEn nuestro caso quedaría de la siguiente forma:\n&lt;target iqn.2020-01.com:tg1&gt;\n    backing-store /dev/vgCliente1/lv1\n&lt;/target&gt;\n\n\nTenemos que reiniciar el servicio de tgt para que reconzca la modificación del fichero /etc/tgt/targets.conf\n\nReiniciamos el servicio\nsudo systemctl restart tgt.service \n\n\nComprobamos que se ha creado correctamente\nsudo tgtadm --mode target --op show\nTarget 1: iqn.2020-01.com:tg1\n    System information:\n        Driver: iscsi\n        State: ready\n    I_T nexus information:\n    LUN information:\n        LUN: 0\n            Type: controller\n            SCSI ID: IET     00010000\n            SCSI SN: beaf10\n            Size: 0 MB, Block size: 1\n            Online: Yes\n            Removable media: No\n            Prevent removal: No\n            Readonly: No\n            SWP: No\n            Thin-provisioning: No\n            Backing store type: null\n            Backing store path: None\n            Backing store flags: \n        LUN: 1\n            Type: disk\n            SCSI ID: IET     00010001\n            SCSI SN: beaf11\n            Size: 734 MB, Block size: 512\n            Online: Yes\n            Removable media: No\n            Prevent removal: No\n            Readonly: No\n            SWP: No\n            Thin-provisioning: No\n            Backing store type: rdwr\n            Backing store path: /dev/vgCliente1/vol1\n            Backing store flags: \n    Account information:\n    ACL information:\n        ALL\n\n\nConfiguración en el cliente\n\nPara iniciar iSCSI en el cliente hay que instalar el paquete open-iscsi:\n\nDescargamos e instalamos open-iscsi\nsudo apt install open-iscsi\n\n\nA continuación se edita el fichero /etc/iscsi/iscsid.conf para indicar que de forma automática escanee los target accesibles al cliente, para eso, tenemos que configurrar el parámetro iscsid.startup en automatic.\n\nModificamos la línea\niscsid.startup = automatic\n\n\nPara que se actualicen los cambios tenemos que reiniciar el servicio de open-iscsi.\n\nReiniciamos el servicio\nsudo systemctl restart open-iscsi.service \n\n\nAhora vamos a buscar los target disponibles y accesibles del Servidor. Para listar los target tenemos que indicarle la dirección del Servidor.\n\nListamos los target\nsudo iscsiadm -m node\n  192.168.100.1:3260,1 iqn.2020-01.com:tg1\n\n\nTenemos el listado de los target disponibles del servidor, solo nos queda conectarnos al target que queramos.\n\nNos conectamos al target iqn.2020-01.com:tg1\nsudo iscsiadm -m node -T iqn.2020-01.com:tg1 --portal \"192.168.100.1\" --login\n  Logging in to [iface: default, target: iqn.2020-01.com:tg1, portal: 192.168.100.1,3260] (multiple)\n  Login to [iface: default, target: iqn.2020-01.com:tg1, portal: 192.168.100.1,3260] successful.\n\n\nPodemos comprobar que se nos ha asignado un nuevo disco de 500MB, el cual es el LUM1 que tenemos en el Servidor.\nComprobamos que tenemos el nuevo disco\nlsblk -l\n  NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n  sda    8:0    0 19.8G  0 disk \n  sda1   8:1    0 18.8G  0 part /\n  sda2   8:2    0    1K  0 part \n  sda5   8:5    0 1021M  0 part [SWAP]\n  sdb    8:16   0  500M  0 disk \n\n\nAhora vamos a particionarlo, formatearlo y montarlo como un disco normal.\n\nLo particionamos con fdisk y le indicamos que sea una particion completa y comprobamos que se ha realizado correctamente con fdisk -l al ser de tipo VIRTUAL_DISK\nParticionamos\nsudo fdisk /dev/sdb\n\n  Welcome to fdisk (util-linux 2.33.1).\n  Changes will remain in memory only, until you decide to write them.\n  Be careful before using the write command.\n\n  Device does not contain a recognized partition table.\n  Created a new DOS disklabel with disk identifier 0x0f7ae951.\n\n  Command (m for help): n\n  Partition type\n     p   primary (0 primary, 0 extended, 4 free)\n     e   extended (container for logical partitions)\n  Select (default p): \n\n  Using default response p.\n  Partition number (1-4, default 1): \n  First sector (2048-1023999, default 2048): \n  Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-1023999, default 1023999): \n\n  Created a new partition 1 of type 'Linux' and of size 499 MiB.\n\n  Command (m for help): w\n  The partition table has been altered.\n  Calling ioctl() to re-read partition table.\n  Syncing disks.\n\nsudo fdisk -l /dev/sdb\n  Disk /dev/sdb: 500 MiB, 524288000 bytes, 1024000 sectors\n  Disk model: VIRTUAL-DISK    \n  Units: sectors of 1 * 512 = 512 bytes\n  Sector size (logical/physical): 512 bytes / 512 bytes\n  I/O size (minimum/optimal): 512 bytes / 512 bytes\n  Disklabel type: dos\n  Disk identifier: 0x0f7ae951\n\n  Device     Boot Start     End Sectors  Size Id Type\n  /dev/sdb1        2048 1023999 1021952  499M 83 Linux\n\n\nFormateandolo a ext4\nsudo mkfs.ext4 /dev/sdb1\n  mke2fs 1.44.5 (15-Dec-2018)\n  Creating filesystem with 510976 1k blocks and 128016 inodes\n  Filesystem UUID: b7296407-b693-45c3-9d26-163e4aba760c\n  Superblock backups stored on blocks: \n  \t8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409\n\n  Allocating group tables: done\n  Writing inode tables: done\n  Creating journal (8192 blocks): done\n  Writing superblocks and filesystem accounting information: done \n\n\nMontandolo en /mnt\nsudo mount /dev/sdb1 /mnt\n\nlsblk -f\n  NAME   FSTYPE LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINT\n  sda                                                                     \n  ├─sda1 ext4         b9ffc3d1-86b2-4a2c-a8be-f2b2f4aa4cb5   16.2G     7% /\n  ├─sda2                                                                  \n  └─sda5 swap         f8f6d279-1b63-4310-a668-cb468c9091d8                [SWAP]\n  sdb                                                                     \n  └─sdb1 ext4         b7296407-b693-45c3-9d26-163e4aba760c    444M     0% /mnt\n\n\nComo podemos ver, el disco nuevo que se ha asignado al cliente, lo podemos utilizar como cualquier otro de manear normal. Ahora el único problema que tenemos que si reiniciamos la máquina el disco se desmontaría y desaparecería de nuestra máquina, para que esto no ocurra vamos a configurar un automontaje en la Tarea 2.\n\nTarea 2\n\nVamos a utilizar systemd mount para que el target se monte automáticamente al arrancar el cliente.\n\nPara realizar el montaje automáticamente tenemos que crear una unidad de systemd para que se monte el disco de manera automática al inciar el equipo del cliente pero para que esto funcione, tenemos que configurar primero iSCSI para realice el login de manera automática al iniciar el cliente.\n\nPara que realice la sesión con el Servidor tenemos que modificar el parámetro node.startup = automatic en el fichero /etc/iscsi/iscsid.conf o podemos ejecutar un comando para modificar dicho parámetro.\n\nEjecutamos el comando para poner el login en automático\nsudo iscsiadm -m node -T iqn.2020-01.com:tg1 -o update -n node.startup -v automatic\n\n\nAhora que tenemos el autologin activado podemos crear la unidad de systemd y que monte el disco de manera automática. Vamos a crear un fichero en /etc/systemd/system, y el fichero se tiene que llamar como el punto de montaje seguido de .mount.\n\nNosotros vamos a montar el disco en /Disco1 por lo tanto el fichero se deberá de llamar Disco1.mount\n\nCreamos el directorio y el fichero\nsudo mkdir /Disco1\nsudo nano /etc/systemd/system/Disco1.mount\n\n\nDentro del fichero vamos a definir la unidad, el punto de montaje y el tipo.\n\nAñadimos las siguiente lineas al fichero /etc/systemd/system/Disco1.mount\n[Unit]\nDescription=Montaje de Disco1\n\n[Mount]\nWhat=/dev/sdb1\nWhere=/Disco1\nType=ext4\nOptions=_netdev\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n  Description: Descripción de la utilidad de la unidad.\n\n  What: La partición que queremos montar /dev/sdb1.\n\n  Where: La ruta donde queremos montar la partición, en nuestro caso /Disco1.\n\n  Type: Le indicamos que el sistema de fichero es ext4 como la partición /dev/sdb1.\n\n  Options: Le índicamos que inicie la unidad de montaje después de que la red este disponible con _netdev..\n\n  WantedBy: Define el estado del sistema de nivel 2.\n\n\nAhora solo nos queda reiniciar el daemon de systemd para que detecte los nuevos cambios y detecte la nueva unidad de .mount.\n\nReiniciamos el demonio\nsudo systemctl daemon-reload \n\n\nIniciamos la unidad creada de Disco1.mount y la habilitamos para que inicie automáticamente al inicio del sistema.\n\nIniciamos y habilitar la unidad Disco1.mount\nsudo systemctl start Disco1.mount\nsudo systemctl enable Disco1.mount\n  Created symlink /etc/systemd/system/multi-user.target.wants/Disco1.mount → /etc/systemd/system/Disco1.mount.\n\n\nAhora comprobamos que al reinciar el cliente se nos monta de manera automática en Disco1.\n\nComprobamación\nvagrant@Cliente1:~$ sudo reboot\n  Connection to 127.0.0.1 closed by remote host.\n  Connection to 127.0.0.1 closed.\n\nmoralg@padano:~/Vagrant/iSCSI$ vagrant ssh cliente1 \n  Linux Cliente1 4.19.0-6-amd64 #1 SMP Debian 4.19.67-2+deb10u2 (2019-11-11) x86_64\n\n  The programs included with the Debian GNU/Linux system are free software;\n  the exact distribution terms for each program are described in the\n  individual files in /usr/share/doc/*/copyright.\n\n  Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n  permitted by applicable law.\n  Last login: Sun Feb  2 18:52:38 2020 from 10.0.2.2\n\nvagrant@Cliente1:~$ lsblk -l\n  NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n  sda    8:0    0 19.8G  0 disk \n  sda1   8:1    0 18.8G  0 part /\n  sda2   8:2    0    1K  0 part \n  sda5   8:5    0 1021M  0 part [SWAP]\n  sdb    8:16   0  500M  0 disk \n  sdb1   8:17   0  499M  0 part /Disco1\n\n\nTarea 3\n\nCrearemos un target con 2 LUN y autenticación por CHAP y la conectaremos a un cliente windows. ¿Cómo se escanea la red en windows y cómo se utilizan las unidades nuevas (formateándolas con NTFS)?\n\nAhora vamos a definir los volúmenes físicos sdc y sdd, crearemos el grupo de volúmenes vgCliente2 y por último crearemos el volumen lógico de 1GB y otro de 500MV para crear los 2 LUN.\n\nDefiniendo el volumen físico\nsudo pvcreate /dev/sdc /dev/sdd\n  Physical volume \"/dev/sdc\" successfully created.\n  Physical volume \"/dev/sdd\" successfully created.\n\n\nCreando el grupo de volúmenes\nsudo vgcreate vgCliente2 /dev/sdc /dev/sdd\n  Volume group \"vgCliente2\" successfully created\n\n\nCreando los volúmenes lógico\nsudo lvcreate -L 500M -n lv2 vgCliente2\n  Logical volume \"lv2\" created.\n\nsudo lvcreate -L 1G -n lv3 vgCliente2\n  Logical volume \"lv3\" created.\n\n\nComprobamos que se ha creado correctamente\nsudo lvs\n  LV   VG         Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n  lv1  vgCliente1 -wi-ao---- 500.00m                                                    \n  lv2  vgCliente2 -wi-a----- 500.00m                                                    \n  lv3  vgCliente2 -wi-a-----   1.00g                                                    \n\nsudo lvdisplay\n  --- Logical volume ---\n  LV Path                /dev/vgCliente2/lv2\n  LV Name                lv2\n  VG Name                vgCliente2\n  LV UUID                pIM3VO-oUwc-AOVO-uwS6-FYlW-tX5T-g7FGPa\n  LV Write Access        read/write\n  LV Creation host, time Servidor1, 2020-02-02 20:00:35 +0000\n  LV Status              available\n  # open                 0\n  LV Size                500.00 MiB\n  Current LE             125\n  Segments               1\n  Allocation             inherit\n  Read ahead sectors     auto\n  - currently set to     256\n  Block device           254:1\n   \n  --- Logical volume ---\n  LV Path                /dev/vgCliente2/lv3\n  LV Name                lv3\n  VG Name                vgCliente2\n  LV UUID                d59SdU-rhC5-D9mS-6Os4-UVwi-zThl-CwHqxZ\n  LV Write Access        read/write\n  LV Creation host, time Servidor1, 2020-02-02 20:00:49 +0000\n  LV Status              available\n  # open                 0\n  LV Size                1.00 GiB\n  Current LE             256\n  Segments               2\n  Allocation             inherit\n  Read ahead sectors     auto\n  - currently set to     256\n  Block device           254:2\n   \n  --- Logical volume ---\n  LV Path                /dev/vgCliente1/lv1\n  LV Name                lv1\n  VG Name                vgCliente1\n  LV UUID                xR20wc-5YFN-af3g-UQGD-LH6Y-MoUL-3F7H2G\n  LV Write Access        read/write\n  LV Creation host, time Servidor1, 2020-02-01 12:30:02 +0000\n  LV Status              available\n  # open                 1\n  LV Size                500.00 MiB\n  Current LE             125\n  Segments               1\n  Allocation             inherit\n  Read ahead sectors     auto\n  - currently set to     256\n  Block device           254:0\n\n\nAhora tenemmos que crear el target con usuario y contraseña en incominguser, que se configura el fichero /etc/tgt/targets.conf con la siguiente configuración:\n\nCreamos el target con los dos LUM\n&lt;target iqn.2020-01.com:tg2&gt;\n    backing-store /dev/vgCliente2/lv2\n    backing-store /dev/vgCliente2/lv3\n    incominguser usuario1 UsuarioPassSec\n&lt;/target&gt;\n\n\n\n  NOTA: La contraseña debe de tener entre 12 y 16 caracteres para que no se error.\n\n\nTenemos que reiniciar el servicio de tgt para que reconzca la modificación del fichero /etc/tgt/targets.conf\n\nReiniciamos el servicio\nsudo systemctl restart tgt.service \n\n\nComprobamos que se ha creado correctamente\nsudo tgtadm --mode target --op show | egrep -A 50 \"iqn.2020-01.com:tg2\"\nTarget 2: iqn.2020-01.com:tg2\n    System information:\n        Driver: iscsi\n        State: ready\n    I_T nexus information:\n    LUN information:\n        LUN: 0\n            Type: controller\n            SCSI ID: IET     00020000\n            SCSI SN: beaf20\n            Size: 0 MB, Block size: 1\n            Online: Yes\n            Removable media: No\n            Prevent removal: No\n            Readonly: No\n            SWP: No\n            Thin-provisioning: No\n            Backing store type: null\n            Backing store path: None\n            Backing store flags: \n        LUN: 1\n            Type: disk\n            SCSI ID: IET     00020001\n            SCSI SN: beaf21\n            Size: 524 MB, Block size: 512\n            Online: Yes\n            Removable media: No\n            Prevent removal: No\n            Readonly: No\n            SWP: No\n            Thin-provisioning: No\n            Backing store type: rdwr\n            Backing store path: /dev/vgCliente2/lv2\n            Backing store flags: \n        LUN: 2\n            Type: disk\n            SCSI ID: IET     00020002\n            SCSI SN: beaf22\n            Size: 1074 MB, Block size: 512\n            Online: Yes\n            Removable media: No\n            Prevent removal: No\n            Readonly: No\n            SWP: No\n            Thin-provisioning: No\n            Backing store type: rdwr\n            Backing store path: /dev/vgCliente2/lv3\n            Backing store flags: \n    Account information:\n        usuario1\n    ACL information:\n\n\nAhora vamos a iniciar el segundo target creado en una máquina Window, para realizar esto vamos abrir el Iniciador iSCSI.\n\nAbrimos el Iniciador iSCSI\n\n\nEn la pestaña Detección vamos a seleccionar en Detectar portal.. e indicamos la dirección de nuestro servidor y el puerto.\n\n\n\nDetectamos el Servidor\n\n\nSi no ha salido ningun mensaje, es que ha dectectado el Servidor y si vamos a la pestaña Destino nos saldrá los target disponibles.\n\nVemos los target disponibles\n\n\nNos sale que el target iqn.2020-01.com:tg2 esta incativo, para activarlo tenemos que darle a Conectar.\n\nNos saltará una ventana donde antes de darle a Aceptar vamos a Activar el inicio de sesión CHAP dandole a Opciones Avanzadas….\n\nSeleccionamos opciones avazadas\n\n\nActivamos el inicio de sesion CHAP y ponemos el usuario y contraseña, los mismo que asignamos en la creación del target.\n\nHabilitamos el inicio de CHAP y introducciomos el usuario y contraseña\n\n\nAhora al darle a Aceptar nos saldrá en estado Conectado.\n\nComprobamos que esta conectado\n\n\nAhora ya tenemos los discos asignado y disponibles para inicializarlos y formatearlos a NTFS. Para hacer esto vamos a abrir Administración de discos\n\nAbrimos el administración de discos\n\n\nInicializamos los discos\n\n\nFormateamos los discos a NTFS\n\n\n\nComprobamos que nos salen montados y podemos utilizarlos de manera normal.\n\nComprobamos que aparecen montados\n\n\nCreamos un fichero\n\n",
        "url": "/sistemas/2020/01/24/Trabajando_con_iSCSI/"
      },
    
      {
        "title": "Introducción a la integración continua",
        "excerpt": "Vamos a realizar una integración continua con Travis con la ayuda de django.\n",
        "content": "\n\nVamos a realizar una integración continua con Travis con la ayuda de django.\n\nTarea 1: Despliegue de una página web estática (build, deploy)\n\nGenerar una página web estática con un generador de paginas estáticas y desplegarla.\n\n\n  En el repositorio GitHub sólo tienen que estar los ficheros markdown.\n  La página se debe generar en el sistema de integración continúa, por lo tanto debemos instalar las herramientas necesarias.\n  Investiga si podemos desplegar de forma automática en el servicio elegido (si es necesario cambia el servicio de hosting para el despliegue).\n\n\nTenemos en el directorio Alejandro-MG de nuestro equipo, la configuración de nuestra página generada con Jekyll (Si quieres saber como instalar y configurar Jekyll pulse AQUÍ), esta la vamos a subir a un repositorio de Github para que Travis la detecte y pueda realizar la integración continua.\n\nPero antes vamos a crea el fichero .gitignore en el directorio para indicar que fichero o directorios no queremos subir. Tenemos que indicarle que no suba a GitHub el directorio donde se generan los html, en nuestro caso _site.\n\ncat .gitignore \n    _site/\n    .sass-cache/\n    .jekyll-metadata\n    alembic-jekyll-theme-*.gem\n    Gemfile.lock\n    **/Gemfile.lock\n\n\n\n  NOTA: Los demás fichero y directorios a parte del _site los he metido porque no son necesario para la generación de la página estática.\n\n\nAhora vamos a subir el directorio al repositorio creado y en la rama master. El branch master va ser el que contenga todos los fichero de configuración, static, etc..\n\nSubiendo nuestro directorio Alejandro-MG a GitHub\ncd Alejandro-MG\ngit init\ngit add .\ngit commit -m 'Creación del repo'\ngit remote add origin git@github.com:MoralG/Alejandro-MG.git\ngit push -u origin master\n\n\nCuando tengamos subido el directorio, vamos a crear un nuevo branch llamado gh-pages donde subiremos el directorio _site,  que es el que contiene los ficheros html.\n\nCreamos la rama gh-pages\n\n\n\n\n\n\nAdemás tenemos que activar GitHub Pages y si tenemos un Dominio propio se lo indicamos también (Si quieres saer como activar GitHub Pages con dominio propio pulse AQUÍ)\n\nIniciamos en Travis con nuestro Github e indicamos la opción Only select repositories y seleccionamos nuestro repositorio Alejandro-MG.\n\nAñadimos el repositorio Alejandro-MG\n\n\n\n\n\n\nComo podemos ver, ya nos sale agregado a Travis:\n\n\n\n\n\n\n\nAhora tenemos que creamos el fichero .travis.yml en el repositorio, este fichero es la configuración que va a seguir Travis para realizar la integración continua.\n\nAñadimos estos pasos al fichero .travis.yml\nlanguage: ruby\nrvm:\n  - 2.4.0\n\nbefore_install:\n#  - gem update --system\n  - gem install bundler\n  -\nscript:\n  - bundle install\n  - bundle exec jekyll build\n\nnotifications:\n  email: false\n\nbranches:\n  only:\n    - master\n\nenv:\n  global:\n    - NOKOGIRI_USE_SYSTEM_LIBRARIES=true\nsudo: false\n\ndeploy:\n  provider: pages\n  skip_cleanup: true\n  local_dir: _site\n  github_token: $GITHUB_TOKEN\n  on:\n    repo: MoralG/Alejandro-MG\n    on:\n      branch: gh-pages\n  fqdn: www.alejandro-mg.com\n\n\nTambién tenemos que crear el fichero CNAME y añadir nuestro dominio, si tenemos uno comprado.\n\nAñadimos el fichero CNAME\nwww.alejandro-mg.com\n\n\nAhora vamos a generar el Tokens necesario para que Travis pueda realizar la integración continua. Para generar esto tenemos que ir a Settings &gt; Developer settings &gt; Personal access tokens y hacer clic en Generate new token.\n\nCreamos el Tokens\n\n\n\n\n\n\nNos saldrá una serie de opciones. Las cuales tenemos que seleccionar repo y admin:repo_hook para concederle permisos de escritura y lesctura a Travis.\n\nLe concedemos los permisos a Travis\n\n\n\n\n\n\nCuando le demos a Generate token nos saldrá el código del Tokens, el cual tendremos que copiar.\nCopiamos el token\n\n\n\n\n\n\nCon el token copiado nos dirigimos a Travis, al settings del repositorio, y nos vamos al apartado de Enviroment Variables y añadimos un token nuevo.\n\nEn el apartado Nombre le indicamos GITHUB_TOKEN y en el apartado Value pegamos el token, lo demás lo dejamos por defecto y le damos a add.\n\nAñadimos a Travis el Token\n\n\n\n\n\n\nAhora ya tendremos todo listo para que se realice la integración continua, para comprobar funciona tenemos que realizar cambios en nuestra configuración de nuestra página estática y realizar un git push -u origin master. Travis empezará a trabajar y a realizar las pautas que le hemos indicado en el fichero .travis.yml.\n\nMostrando el Job log de Travis\nWorker information\n\nBuild system information\n\n$ git clone --depth=50 --branch=master https://github.com/MoralG/Alejandro-MG.git MoralG/Alejandro-MG\nMoralG/Alejandro-MG\nCloning into 'MoralG/Alejandro-MG'...\n$ cd MoralG/Alejandro-MG\n$ git checkout -qf 4c58780e533a84ca1ad807ae5fea731ac286fc3f\n\n\nSetting environment variables from repository settings\n$ export GITHUB_TOKEN=[secure]\n\nSetting environment variables from .travis.yml\n$ export NOKOGIRI_USE_SYSTEM_LIBRARIES=true\n\nrvm use 2.4.0 --install --binary --fuzzy\n\n$ export BUNDLE_GEMFILE=$PWD/Gemfile\n$ ruby --version\n\ngem install bundler\nbundle install --jobs=3 --retry=3\n\nbundle install\nUsing concurrent-ruby 1.1.5\nUsing i18n 0.9.5\nUsing minitest 5.14.0\nUsing thread_safe 0.3.6\nUsing tzinfo 1.2.6\nUsing activesupport 5.2.4.1\nUsing public_suffix 4.0.3\nUsing addressable 2.7.0\nUsing colorator 1.1.0\nUsing eventmachine 1.2.7\nUsing http_parser.rb 0.6.0\nUsing em-websocket 0.5.1\nUsing rb-fsevent 0.10.3\nUsing ffi 1.12.2\nUsing rb-inotify 0.10.1\nUsing sass-listen 4.0.0\nUsing sass 3.7.4\nUsing jekyll-sass-converter 1.5.2\nUsing listen 3.2.1\nUsing jekyll-watch 2.2.1\nUsing kramdown 1.17.0\nUsing liquid 4.0.3\nUsing mercenary 0.3.6\nUsing forwardable-extended 2.6.0\nUsing pathutil 0.16.2\nUsing rouge 3.15.0\nUsing safe_yaml 1.0.5\nUsing jekyll 3.8.6\nUsing ruby-enum 0.7.2\nUsing commonmarker 0.21.0\nUsing jekyll-commonmark 1.3.1\nUsing jekyll-default-layout 0.1.4\nUsing jekyll-feed 0.13.0\nUsing jekyll-include-cache 0.2.0\nUsing mini_portile2 2.4.0\nUsing nokogiri 1.10.7\nUsing html-pipeline 2.12.3\nUsing jekyll-mentions 1.5.1\nUsing jekyll-paginate 1.1.0\nUsing jekyll-redirect-from 0.16.0\nUsing jekyll-seo-tag 2.6.1\nUsing jekyll-sitemap 0.13.0\nUsing gemoji 3.0.1\nUsing jemoji 0.11.1\nUsing alembic-jekyll-theme 3.1.0 from source at `.`\nUsing bundler 2.1.4\nBundle complete! 1 Gemfile dependency, 46 gems now installed.\nUse `bundle info [gemname]` to see where a bundled gem is installed.\nThe command \"bundle install\" exited with 0.\n$ bundle exec jekyll build\nConfiguration file: /home/travis/build/MoralG/Alejandro-MG/_config.yml\n            Source: /home/travis/build/MoralG/Alejandro-MG\n       Destination: /home/travis/build/MoralG/Alejandro-MG/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n       Jekyll Feed: Generating feed for posts\n                    done in 3.135 seconds.\n Auto-regeneration: disabled. Use --watch to enable.\nThe command \"bundle exec jekyll build\" exited with 0.\n\n$ rvm $(travis_internal_ruby) --fuzzy do ruby -S gem install dpl\n\nInstalling deploy dependencies\nLogged in as @MoralG (Alejandro Morales Gracia)\ncd /tmp/d20200203-7718-1g0d5gj/work\nPreparing deploy\nDeploying application\nDone. Your build exited with 0.\n\n\nTerminada la integración correctamente, Travis nos mostrará el mensaje Done. Your build exited with 0 y se mostrará así:\n\n\n\n\n\n\n\nTarea 2: Integración continúa de aplicación django (Test + Deploy)\n\nVamos a trabajar con el repositorio de la aplicación django_tutorial. Esta aplicación tiene definidas una serie de test, que podemos estudiar en el fichero tests.py del directorio polls.\n\nPara ejecutar las pruebas unitarias, ejecutamos la instrucción python3 manage.py test.\n\n\n  Estudia las distintas pruebas que se han realizado, y modifica el código de la aplicación para que al menos una de ella no se ejecute de manera exitosa.\n\n\nA continuación vamos a configurar la integración continúa para que cada vez que hagamos un commit se haga la ejecución de test en travis.\n\n\n\nVamos a empezar clonando el repositorio indicado django_tutorial en nuestra máquina para poder realizar los test necesarios.\n\nClonamos el repositorio\ngit clone https://github.com/josedom24/django_tutorial.git\n  Clonando en 'django_tutorial'...\n  remote: Enumerating objects: 3, done.\n  remote: Counting objects: 100% (3/3), done.\n  remote: Compressing objects: 100% (2/2), done.\n  remote: Total 96 (delta 0), reused 0 (delta 0), pack-reused 93\n  Desempaquetando objetos: 100% (96/96), listo.\n\n\nCreamos un entorno virtual llamado django_tutorial para poder instalar los requirement.\n\nCreamos el entorno virtual\npython3 -m venv django_tutorial\nsource django_tutorial/bin/activate\n\n\nInstalamos el requirement.txt\npip install -r requirements.txt \n  Collecting Django==2.2.7 (from -r requirements.txt (line 1))\n    Using cached https://files.pythonhosted.org/packages/a0/36/ 463632a2e9161a7e713488d719a280e8cb0c7e3a66ed32a32e801891caae/  Django-2.2.7-py3-none-any.whl\n  Collecting gunicorn (from -r requirements.txt (line 2))\n    Downloading https://files.pythonhosted.org/packages/69/ca/  926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/ gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n      100% |████████████████████████████████| 81kB 587kB/s \n  Collecting pytz (from Django==2.2.7-&gt;-r requirements.txt (line 1))\n    Using cached https://files.pythonhosted.org/packages/e7/f9/ f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/  pytz-2019.3-py2.py3-none-any.whl\n  Collecting sqlparse (from Django==2.2.7-&gt;-r requirements.txt (line 1))\n    Using cached https://files.pythonhosted.org/packages/ef/53/ 900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/  sqlparse-0.3.0-py2.py3-none-any.whl\n  Requirement already satisfied: setuptools&gt;=3.0 in /home/moralg/EntornosVirtuales/ django_tutorial/lib/python3.7/site-packages (from gunicorn-&gt;-r requirements.txt (line  2)) (40.8.0)\n  Installing collected packages: pytz, sqlparse, Django, gunicorn\n  Successfully installed Django-2.2.7 gunicorn-20.0.4 pytz-2019.3 sqlparse-0.3.0\n\n\nAhora vamos a realizar el test para comprobar la integridad del programa django.\n\nRealizamos el test\npython3 manage.py test\n  Creating test database for alias 'default'...\n  System check identified no issues (0 silenced).\n  ..........\n  ----------------------------------------------------------------------\n  Ran 10 tests in 0.031s\n\n  OK\n  Destroying test database for alias 'default'...\n\n\nLa prueba se ha realizado correctamente, pero si añadimos una linea que no sigue las normas de los test del ficherpo polls/test.py, el test fallará.\n\nPor ejemplo, en el fichero test.py nos indica en diferentes reglas, que debe de responder No polls are available..\n\nLinea del fichero test.py\nself.assertContains(response, \"No polls are available.\")\n\n\nSi cambiamos ese respuesta en el fichero polls/templates/polls/index.html deberá de fallar el test\n\nModificamos el fichero index.html\n.\n.\n.\n    &lt;p&gt;ESTO_ES_UNA_PRUEBA_PARA_REALIZAR_EL_TEST&lt;/p&gt;\n.\n.\n.\n\n\n\nEjecutamos el test\npython3 manage.py test\n  Creating test database for alias 'default'...\n  System check identified no issues (0 silenced).\n  ..F.F.....\n  ======================================================================\n  FAIL: test_future_question (polls.tests.QuestionIndexViewTests)\n  ----------------------------------------------------------------------\n  Traceback (most recent call last):\n    File \"/home/moralg/Github/django_tutorial/polls/tests.py\", line 79, in  test_future_question\n      self.assertContains(response, \"No polls are available.\")\n    File \"/home/moralg/EntornosVirtuales/django_tutorial/lib/python3.7/site-packages/ django/test/testcases.py\", line 454, in assertContains\n      self.assertTrue(real_count != 0, msg_prefix + \"Couldn't find %s in response\" %  text_repr)\n  AssertionError: False is not true : Couldn't find 'No polls are available.' in  response\n\n  ======================================================================\n  FAIL: test_no_questions (polls.tests.QuestionIndexViewTests)\n  ----------------------------------------------------------------------\n  Traceback (most recent call last):\n    File \"/home/moralg/Github/django_tutorial/polls/tests.py\", line 57, in  test_no_questions\n      self.assertContains(response, \"No polls are available.\")\n    File \"/home/moralg/EntornosVirtuales/django_tutorial/lib/python3.7/site-packages/ django/test/testcases.py\", line 454, in assertContains\n      self.assertTrue(real_count != 0, msg_prefix + \"Couldn't find %s in response\" %  text_repr)\n  AssertionError: False is not true : Couldn't find 'No polls are available.' in  response\n\n  ----------------------------------------------------------------------\n  Ran 10 tests in 0.027s\n\n  FAILED (failures=2)\n  Destroying test database for alias 'default'...\n\n\nComo podemos observar en el fallo, nos muestras los dos test que depende de que la respuesta tenga que ser No polls are available. y nos salta con el error AssertionError: False is not true : Couldn't find 'No polls are available.' in  response.\n\nAhora vamos a realzar un fork del repositorio django_tutorial para sincronizarlo con Travis y que se comproebe el test.\n\nRealizamos el fork\n\n\nCuando tengamos realicemos el fork, vamos a clonarlo y añadiremos dentro del repositorio un fichero .travis.yml para realizar las reglas de Travis.\n\nClonamos el repositorio y creamos el fichero .travis.yml\ngit clone git@github.com:MoralG/django_tutorial.git\ncd django_tutorial\ntouch .travis.yml\n\n\nEl fichero de Travis vamos a indicarle la versión, la intalación del fichero requirement.txt y que realice el test.\n\nFichero .travis.yml\nlanguage: python\npython:\n  - \"3.8\"\ninstall:\n  - pip3 install -r requirements.txt\nscript: python3 manage.py test\n\n\nAhora que tenemos el repositorio preparado vamos a sincronizarlo a Travis para realizar el ejercicio.\n\n\n\n\n\n\n\nTenemos listo el escenario para realizar el mismo cambio que hicimos en el punto anterior, y que nos notifique Travis que hay un error al realizar los test.\n\n\n\n\n\n\n\nCambio realizado con el error\n\n\n\n\n\n\nEn el Job log nos muestra los test que han fallado y nos devuelve el valor 1. Done. Your build exited with 1.\n\nMostramos el Job log\nBuild system information\n\nDownloading archive: https://storage.googleapis.com/travis-ci-language-archives/python/binaries/ubuntu/16.04/x86_64/python-3.8.tar.bz2\n$ curl -sSf --retry 5 -o python-3.8.tar.bz2 ${archive_url}\n$ sudo tar xjf python-3.8.tar.bz2 --directory /\n\n$ git clone --depth=50 --branch=master https://github.com/MoralG/django_tutorial.git MoralG/django_tutorial\n\n$ source ~/virtualenv/python3.8/bin/activate\n$ python --version\nPython 3.8.0\n$ pip --version\npip 19.3 from /home/travis/virtualenv/python3.8.0/lib/python3.8/site-packages/pip (python 3.8)\n\n$ pip3 install -r requirements.txt\n$ python3 manage.py test\nCreating test database for alias 'default'...\nSystem check identified no issues (0 silenced).\n..F.F.....\n======================================================================\nFAIL: test_future_question (polls.tests.QuestionIndexViewTests)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/travis/build/MoralG/django_tutorial/polls/tests.py\", line 79, in test_future_question\n    self.assertContains(response, \"No polls are available.\")\n  File \"/home/travis/virtualenv/python3.8.0/lib/python3.8/site-packages/django/test/testcases.py\", line 454, in assertContains\n    self.assertTrue(real_count != 0, msg_prefix + \"Couldn't find %s in response\" % text_repr)\nAssertionError: False is not true : Couldn't find 'No polls are available.' in response\n======================================================================\nFAIL: test_no_questions (polls.tests.QuestionIndexViewTests)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/travis/build/MoralG/django_tutorial/polls/tests.py\", line 57, in test_no_questions\n    self.assertContains(response, \"No polls are available.\")\n  File \"/home/travis/virtualenv/python3.8.0/lib/python3.8/site-packages/django/test/testcases.py\", line 454, in assertContains\n    self.assertTrue(real_count != 0, msg_prefix + \"Couldn't find %s in response\" % text_repr)\nAssertionError: False is not true : Couldn't find 'No polls are available.' in response\n----------------------------------------------------------------------\nRan 10 tests in 0.033s\n\nFAILED (failures=2)\nDestroying test database for alias 'default'...\nThe command \"python3 manage.py test\" exited with 1.\n\nDone. Your build exited with 1.\n\n\nCambio corregido y sin fallos\n\n\n\n\n\n\nEn este caso, donde hemos arreglado el error, nos muestra el Job log sin ningun error del test y nos devuelve un valor de 0. Done. Your build exited with 0.\n\nMostrando el Job log\nBuild system information\n\nDownloading archive: https://storage.googleapis.com/travis-ci-language-archives/python/binaries/ubuntu/16.04/x86_64/python-3.8.tar.bz2\ncurl -sSf --retry 5 -o python-3.8.tar.bz2 ${archive_url}\nsudo tar xjf python-3.8.tar.bz2 --directory /\n\n$ git clone --depth=50 --branch=master https://github.com/MoralG/django_tutorial.git MoralG/\n\n$ source ~/virtualenv/python3.8/bin/activate\n$ python --version\nPython 3.8.0\n$ pip --version\nPython 3.8.0\n$ pip 19.3 from /home/travis/virtualenv/python3.8.0/lib/python3.8/site-packages/pip (python 3.8)\n$ pip3 install -r requirements.txt\n$ python3 manage.py test\nCreating test database for alias 'default'...\nSystem check identified no issues (0 silenced).\n..........\n----------------------------------------------------------------------\nRan 10 tests in 0.033s\n\nOK\nDestroying test database for alias 'default'...\nThe command \"python3 manage.py test\" exited with 0.\n\nDone. Your build exited with 0.\n\n",
        "url": "/implantaci%C3%B3n%20app%20web/2020/02/04/Introduccion_de_Integracion_Continua/"
      },
    
      {
        "title": "Métricas, logs y monitorización con Netdata",
        "excerpt": "Instalamos el servicio de Netdata y lo configuramos para recopilar información de nuestros servidores de métricas, logs y monitorización.\n",
        "content": "Instalamos el servicio de Netdata y lo configuramos para recopilar información de nuestros servidores de métricas, logs y monitorización.\n\n\n\nNetdata es una herramienta para visualizar y monitorear métricas en tiempo real. Escrita en C, con un rendimiento muy óptimo y necesita muy pocos recurso durante su ejecucción.\n\nConsiste en un demonio que cuando se ejecuta se encarga de obtener información en tiempo real y presentarla en la web. La recoleción de información utiliza plugins internos y/o externos.\n\nInstalación Netdata\n\nInstalamos las dependencias de netdata.\nsudo apt-get install zlib1g-dev uuid-dev libuv1-dev liblz4-dev libjudy-dev libssl-dev libmnl-dev gcc make git autoconf autoconf-archive autogen automake pkg-config curl python\n\n\nVamos a clonar de GitHub el repositorio donde se aloja la herramienta netdata, para despues instalarla.\n\ngit clone https://github.com/netdata/netdata.git\n\n\nNos dirigimos a la ruta del directorio que se ha clonado e instalamos netdata.\ncd netdata/\nsudo ./netdata-installer.sh\n\n  ^\n  |.-.   .-.   .-.   .-.   .  netdata                                        \n  |   '-'   '-'   '-'   '-'   real-time performance monitoring, done right!  \n  +----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---&gt;\n\n\n  You are about to build and install netdata to your system.\n\n  It will be installed at these locations:\n\n   - the daemon     at /usr/sbin/netdata\n   - config files   in /etc/netdata\n   - web files      in /usr/share/netdata\n   - plugins        in /usr/libexec/netdata\n   - cache files    in /var/cache/netdata\n   - db files       in /var/lib/netdata\n   - log files      in /var/log/netdata\n   - pid file       at /var/run/netdata.pid\n   - logrotate file at /etc/logrotate.d/netdata\n\n  This installer allows you to change the installation path.\n  Press Control-C and run the same command with --help for help.\n\n\n  NOTE:\n  Anonymous usage stats will be collected and sent to Google Analytics.\n  To opt-out, pass --disable-telemetry option to the installer.\n\nPress ENTER to build and install netdata to your system &gt; \n\n\nPulsamos ENTER y comenzara la compilación y posteriormente la instalación de netdata en las rutas que se muestran arriba.\n\nCompletada la instalación nos aparecerá el siguente mensaje:\n --- We are done! --- \n\n  ^\n  |.-.   .-.   .-.   .-.   .-.   .  netdata                          .-.   .-\n  |   '-'   '-'   '-'   '-'   '-'   is installed and running now!  -'   '-'  \n  +----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---&gt;\n\n  enjoy real-time performance and health monitoring...\n\n\nYa tenemos netdata instalado y el demonio esta activado y funcionando.\nsudo systemctl list-unit-files | grep netdata\n  netdata.service                        enabled\n\n\nPodemos ver el estado, y el último log nos índica que ha empezado la monitorización en tiempo real.\nsudo systemctl status netdata.service \n  ● netdata.service - Real time performance monitoring\n     Loaded: loaded (/lib/systemd/system/netdata.service; enabled; vendor preset: enabled)\n     Active: active (running) since Fri 2020-01-24 10:40:10 UTC; 1 weeks 6 days ago\n    Process: 8846 ExecStartPre=/bin/mkdir -p /var/cache/netdata (code=exited, status=0/SUCCESS)\n    Process: 8847 ExecStartPre=/bin/chown -R netdata:netdata /var/cache/netdata (code=exited, status=0/ SUCC\n    Process: 8848 ExecStartPre=/bin/mkdir -p /var/run/netdata (code=exited, status=0/SUCCESS)\n    Process: 8849 ExecStartPre=/bin/chown -R netdata:netdata /var/run/netdata (code=exited, status=0/ SUCCES\n   Main PID: 8850 (netdata)\n      Tasks: 36 (limit: 1168)\n     Memory: 186.1M\n     CGroup: /system.slice/netdata.service\n             ├─  312 bash /usr/libexec/netdata/plugins.d/tc-qos-helper.sh 1\n             ├─ 8850 /usr/sbin/netdata -P /var/run/netdata/netdata.pid -D -W set global process   scheduling \n             ├─ 8901 /usr/bin/python /usr/libexec/netdata/plugins.d/python.d.plugin 1\n             ├─ 8907 /usr/libexec/netdata/plugins.d/go.d.plugin 1\n             └─32540 /usr/libexec/netdata/plugins.d/apps.plugin 1\n\n  Jan 24 10:40:10 serranito systemd[1]: Starting Real time performance monitoring...\n  Jan 24 10:40:10 serranito systemd[1]: Started Real time performance monitoring.\n\n\nConfiguración del Servidor (Serranito)\nYa tenemos funcionando netdata en el Servidor Serranito, pero no podemos visualizar la monitorización ni las métricas, ya que no tenemos un servidor web para mostrar la herramienta.\n\nVamos a instalar apache:\nsudo apt-get install apache2-bin\n\n\nActivamos los módulos proxy y proxy_http, necesarios ya que el puerto de netdata es el 19999.\nsudo a2enmod proxy\nsudo a2enmod proxy_http\n\n\nActivamos también el módulo rewrite.\nsudo a2enmod rewrite\n\n\nCreamos el virtualhost de apache, para la aplicación web de netdata.\nnano /etc/apache2/sites-available/netdata.conf\n\n\nFichero netdata.conf.\n&lt;VirtualHost *:80&gt;\n    RewriteEngine On\n    ProxyRequests Off\n    ProxyPreserveHost On\n\n    ServerName netdata.amorales.gonzalonazareno.org\n\n    &lt;Proxy *&gt;\n        Require all granted\n    &lt;/Proxy&gt;\n\n    ProxyPass \"/\" \"http://localhost:19999/\" connectiontimeout=5 timeout=30 keepalive=on\n    ProxyPassReverse \"/\" \"http://localhost:19999/\"\n\n    ErrorLog ${APACHE_LOG_DIR}/netdata-error.log\n    CustomLog ${APACHE_LOG_DIR}/netdata-access.log combined\n&lt;/VirtualHost&gt;\n\n\nActivamos el virtualhost creando el enlace símbolico.\na2ensite netdata\n\n\nReiniciamos los servicios de netdata y apache2.\nsystemctl restart netdata.service\nsystemctl restart apache2.service\n\n\nAñadimos DNS el registro al DNS del Servidor Croqueta.\nnetdata   IN   CNAME   serranito\n\n\nComprobamos que funciona la página.\n\n\nConfiguración de los Clientes (Croqueta, Tortilla, Salmorejo)\n\nAhora vamos a recoger los datos, para la monitorización y las métricas, de los clientes. Para realizar esto, tenemos que configurar a los clientes como esclavos, que se realiza con un fichero de configuración.\n\nTenemos que generar una UUID para que reconozca el servidor al cliente a través de la apy key. Con el paquete uuid-runtime podemos generar un uuid.\nsudo apt install uuid-runtime\n\n\nGeneramos la uuid con el comando uuidgen.\nuuidgen\n  4c97ef68-ca16-41e9-b5df-d7f017d9da8e\n\n\nCreamos el fichero /etc/netdata/stream.conf y añadimos la configuración del maestro para que pueda escuchar a los clientes.\n# -----------------------------------------------------------------------------\n# 1. EN LA MÁQUINA SLAVE NETDATA - SOLO PARA ENVIAR MÉTRICAS\n\n[stream]\n    enabled = no\n    destination =\n    api key = \n    timeout seconds = 60\n    default port = 19999\n    buffer size bytes = 1048576\n    reconnect delay seconds = 5\n    initial clock resync iterations = 60\n\n# -----------------------------------------------------------------------------\n# 2-1. EN LA MÁQUINA MASTER NETDATA - SOLO PARA RECIBIR MÉTRICAS CROQUETA\n\n[4c97ef68-ca16-41e9-b5df-d7f017d9da8e]\n    enabled = yes\n    allow from = *\n    default history = 3600\n    default memory mode = ram\n    health enabled by default = auto\n    default postpone alarms on connect seconds = 60\n\n# -----------------------------------------------------------------------------\n# 2-2. EN LA MÁQUINA MASTER NETDATA - SOLO PARA RECIBIR MÉTRICAS TORTILLA\n\n[415e58a5-b1de-41e4-8eac-d749bc572df5]\n    enabled = yes\n    allow from = *\n    default history = 3600\n    default memory mode = ram\n    health enabled by default = auto\n    default postpone alarms on connect seconds = 60\n\n# -----------------------------------------------------------------------------\n# 2-3. EN LA MÁQUINA MASTER NETDATA - SOLO PARA RECIBIR MÉTRICAS SALMOREJO\n\n[7a4c6dd7-d71a-46ba-b1d5-150333707ff7]\n    enabled = yes\n    allow from = *\n    default history = 3600\n    default memory mode = ram\n    health enabled by default = auto\n    default postpone alarms on connect seconds = 60\n\n# -----------------------------------------------------------------------------\n# 3. PER SENDING HOST SETTINGS, ON MASTER NETDATA THIS IS OPTIONAL - YOU DON'T NEED IT\n\n[MACHINE_GUID]\n    enabled = no\n    allow from = *\n    history = 3600\n    memory mode = save\n    health enabled = yes\n    postpone alarms on connect seconds = 60\n\n\nUna vez creado el fichero stream.conf podemos reiniciar el servicio netdata.service del servidor para que empiece a escuchar.\n\nComo podemos ver al mirar el log del maestro, esta escuchando pero nadie responde y cierra la conexión.\n\ntail -f /var/log/netdata/error.log\n  2020-02-07 21:47:42: netdata INFO  : WEB_SERVER[static1] : POLLFD: LISTENER: client slot 5 (fd 82) from localhost port 43156 is idle for more than 60 seconds - closing it. \n\n\nPara que el maestro escuche a los clientes, estos los vamos a configurar como esclavos. Para esto vamos a instalar netdata como hicimos en el servidor. IR ARRIBA\n\nCreamos el fichero /etc/netdata/stream.conf y añadimos la configuración del esclavo en las máquinas Croqueta, Salmorejo y Tortilla.\n# -----------------------------------------------------------------------------\n# 1. ON SLAVE NETDATA - THE ONE THAT WILL BE SENDING METRICS\n\n[stream]\n    enabled = yes\n    destination = 10.0.0.17\n    api key = 4c97ef68-ca16-41e9-b5df-d7f017d9da8e\n    timeout seconds = 60\n    default port = 19999\n    buffer size bytes = 1048576\n    reconnect delay seconds = 5\n    initial clock resync iterations = 60\n\n# -----------------------------------------------------------------------------\n# 2. ON MASTER NETDATA - THE ONE THAT WILL BE RECEIVING METRICS\n\n[API_KEY]\n    enabled = no\n    allow from = *\n    default history = 3600\n    default memory mode = ram\n    health enabled by default = auto\n    default postpone alarms on connect seconds = 60\n\n# -----------------------------------------------------------------------------\n# 3. PER SENDING HOST SETTINGS, ON MASTER NETDATA THIS IS OPTIONAL - YOU DON'T NEED IT\n\n[MACHINE_GUID]\n    enabled = no\n    allow from = *\n    history = 3600\n    memory mode = save\n    health enabled = yes\n    postpone alarms on connect seconds = 60\n\n\nDespués reiniciamos el servicio netdata.service de los esclavos y al mirar el log nos saldrá los siguiente mensajes\n\nsystemctl restart netdata.service\n\n\nServidor\ntail -f /var/log/netdata/error.log | egrep STREAM\n  2020-02-07 21:54:23: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : STREAM croqueta   [receive from [10.0.0.6]:54704]: initializing communication...\n  2020-02-07 21:54:23: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : STREAM croqueta   [receive from [10.0.0.6]:54704]: Netdata is using the newest stream protocol.\n  2020-02-07 21:54:23: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : Postponing health   checks for 60 seconds, on host 'croqueta', because it was just connected.\n  2020-02-07 21:54:23: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : STREAM croqueta   [receive from [10.0.0.6]:54704]: receiving metrics...\n  2020-02-07 21:54:30: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'net.eth0' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.threads' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.processes' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.uptime' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.uptime_min' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.uptime_avg' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.uptime_max' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.mem' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.vmem' on host 'croqueta' already exists.\n  2020-02-07 21:54:47: netdata INFO  : STREAM_RECEIVER[croqueta,[10.0.0.6]:54704] : RRDSET: chart name  'groups.swap' on host 'croqueta' already exists.\n\n\nCliente\n\nsystemctl restart netdata.service\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.swap' on host  'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.major_faults' on host  'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.minor_faults' on host  'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.preads' on host  'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.pwrites' on host   'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.lreads' on host  'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.lwrites' on host   'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.files' on host   'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.sockets' on host   'croqueta' already exists.\n  2020-02-07 21:55:13: netdata INFO  : PLUGINSD[apps] : RRDSET: chart name 'groups.pipes' on host   'croqueta' already exists.\n\n\nComprobamos en la aplicación que nos aparece los clientes de Croqueta, Tortilla y Salmorejo.\n\n\nYa tenemos todos los datos de los clientes, agrupados en el servidor y nos lo muestra en nuestra servicio web, pero nos muestra una gran cantidad de métricas que son innecesarias para la funcionalidad que nosotros queremos.\n\nAhora vamos quitar toda esas métricas sobrantes que nosotros no necesitamos y dejar las más importantes para nosotros, para realizar esto vamos a modificar el fichero /etc/netdata/netdata.conf. En este fichero tenenmos todos los campos del Dashboard, los cuales podemos deshabilitar con la opción enable = no, como se muestra en el ejemplo del apartado de las variables de entropia:\n[system.entropy]\n        history = 5\n        enabled = no\n        # cache directory = /var/cache/netdata/system.entropy\n        # chart type = line\n        # type = system\n        # family = entropy\n        # units = entropy\n        # context = system.entropy\n        # priority = 1000\n        # name = system.entropy\n        # title = Available Entropy\n        # dim entropy name = entropy\n        # dim entropy algorithm = absolute\n        # dim entropy multiplier = 1\n        # dim entropy divisor = 1\n\n\nDeshabilitado, ya no se mostrará en la web.\n\nAdemás de dejar el Dashboard funcional para nosotros, podemos configurar las notificaciones. Estas notificaciones de las que estoy hablando, son las encargada de avisarte con una alerta si algun valor de los que esta midiendo sobrepasa el límite indicado.\n\nPara modificar las notificaciones tenemos que utilizar el script /etc/netdata/edit-config e indicarle el fichero donde este la configuración de la notificación que queramos modificar. Por ejemplo en nuestro caso vamos a modificar la notificación de la CPU.\n\nsudo ./edit-config health.d/cpu.conf\n\n\nVamos a modificar la alerta del WARNING para que salte cuando la CPU llegue al 65%.\n\nAdemás, si nos fijamos en el usuario asignado para esta notificación, en el apartado to podemos \nasignar el usuario silent y no nos avisará de las alertas.\n\ntemplate: 10min_cpu_usage\n      on: system.cpu\n      os: linux\n   hosts: *\n  lookup: average -10m unaligned of user,system,softirq,irq,guest\n   units: %\n   every: 1m\n    warn: $this &gt; (($status &gt;= $WARNING)  ? (65) : (85))\n    crit: $this &gt; (($status == $CRITICAL) ? (85) : (95))\n   delay: down 15m multiplier 1.5 max 1h\n    info: average cpu utilization for the last 10 minutes (excluding iowait, nice and steal)\n      to: sysadmin\n\n\n\n  NOTA: sysadmin puede mandar la alerta a discord o slack, esto lo explicaremos más abajo\n\n\nGuardamos la configuración y ya nos saldrá configurado en nuestra web.\n\n\nPara probar lo de las notificaciones vamos a estresar nuestra máquina. Vamos a instalar el paquete de stress y esperaremos a que la utilización de la CPU aumente hasta el 60%.\n\nsudo apt install stress\n\n\nEstresamos la máquina\nstress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 200s\n  stress: info: [685] dispatching hogs: 8 cpu, 4 io, 2 vm, 0 hdd\n\n\nComo podemos ver, la CPU esta al 100%:\n\n\nY tenemos las notificaciones que nos avisan:\n\n\nEsto esta muy bien, pero en el caso de no estar atento a la web, tenemos que buscar un metodo para que nos notifiquen, por eso vamos a configurar netdata para que nos envien las alertas a discord y slack.\n\nDiscord\n\nPara configurar las notificaciones en discord tenemos que crear un servidor en discord y avtivar la opción de Webhook\n\n\nNos dará un enlace, el cual añadiremos a la configuración de netdata en el fichero health_alarm_notify.conf con el script edit-config.\n\nsudo ./edit-config health_alarm_notify.conf\n  # discord (discordapp.com) global notification options\n\n  # multiple recipients can be given like this:\n  #                  \"CHANNEL1 CHANNEL2 ...\"\n\n  # enable/disable sending discord notifications\n  SEND_DISCORD=\"YES\"\n\n  # Create a webhook by following the official documentation -\n  # https://support.discordapp.com/hc/en-us/articles/228383668-Intro-to-Webhooks\n  DISCORD_WEBHOOK_URL=\"https://discordapp.com/api/webhooks/676357546080206848/  ePWMIzAJd-YsjyacjHc01xun_SV6E\n\n  # if a role's recipients are not configured, a notification will be send to\n  # this discord channel (empty = do not send a notification for unconfigured\n  # roles):\n  DEFAULT_RECIPIENT_DISCORD=\"alarms\"\n\n\nComo podemos ver, en DISCORD_WEBHOOK_URL añadiremos la url proporcionada al activar Webhook y en DEFAULT_RECIPIENT_DISCORD le indicamos alarms.\n\nLe realizamos el mismo comando de antes para estresarlo y nos avisará.\n\n\nSlack\n\nVamos a configurar como en el caso de discord, el fichero health_alarm_notify.conf con el script edit-config. Y añadimos a SLACK_WEBHOOK_URL el enlace que nos da Webhook al crearlo en nuestro servidor.\n\n\n\nAdemás le indicamos en DEFAULT_RECIPIENT_SLACK que sea # para que reconozca el canal que le hemos indicado en el Webhook.\nsudo ./edit-config health_alarm_notify.conf\n  # slack (slack.com) global notification options\n\n  # multiple recipients can be given like this:\n  #                  \"RECIPIENT1 RECIPIENT2 ...\"\n\n  # enable/disable sending slack notifications\n  SEND_SLACK=\"YES\"\n\n  # Login to your slack.com workspace and create an incoming webhook, using the \"Incoming Webhooks\" App:  ht\n  # Do not use the instructions in https://api.slack.com/incoming-webhooks#enable_webhooks, as those  webhoo\n  # You need only one for all your netdata servers (or you can have one for each of your netdata).\n  # Without the app and a webhook, netdata cannot send slack notifications.\n  SLACK_WEBHOOK_URL=\"https://hooks.slack.com/services/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\"\n\n  # if a role's recipients are not configured, a notification will be send\n  # to:# - A slack channel (syntax: '#channel' or 'channel')\n  # - A slack user (syntax: '@user')\n  # - The channel or user defined in slack for the webhook (syntax: '#')\n  # empty = do not send a notification for unconfigured roles\n  DEFAULT_RECIPIENT_SLACK=\"#\"\n\n\nNos conectamos al usuario netdata\nsudo su -s /bin/bash netdata\n\n\nProbamos que funciona con un test que se ejecuta con el siguiente script:\n/usr/libexec/netdata/plugins.d/alarm-notify.sh test\n\n  # SENDING TEST WARNING ALARM TO ROLE: sysadmin\n  /etc/netdata/health_alarm_notify.conf: line 435: Colors: command not found\n  2020-02-10 18:30:02: alarm-notify.sh: INFO: sent slack notification for: serranito test.chart.  test_alarm is WARNING without specifying a channel\n  2020-02-10 18:30:02: alarm-notify.sh: INFO: sent discord notification for: serranito test.chart.  test_alarm is WARNING to 'alarms'\n  2020-02-10 18:30:03: alarm-notify.sh: INFO: sent email notification for: serranito test.chart.  test_alarm is WARNING to 'root'\n  # OK\n\n  # SENDING TEST CRITICAL ALARM TO ROLE: sysadmin\n  /etc/netdata/health_alarm_notify.conf: line 435: Colors: command not found\n  2020-02-10 18:30:03: alarm-notify.sh: INFO: sent slack notification for: serranito test.chart.  test_alarm is CRITICAL without specifying a channel\n  2020-02-10 18:30:03: alarm-notify.sh: INFO: sent discord notification for: serranito test.chart.  test_alarm is CRITICAL to 'alarms'\n  2020-02-10 18:30:03: alarm-notify.sh: INFO: sent email notification for: serranito test.chart.  test_alarm is CRITICAL to 'root'\n  # OK\n\n  # SENDING TEST CLEAR ALARM TO ROLE: sysadmin\n  /etc/netdata/health_alarm_notify.conf: line 435: Colors: command not found\n  2020-02-10 18:30:04: alarm-notify.sh: INFO: sent slack notification for: serranito test.chart.  test_alarm is CLEAR without specifying a channel\n  2020-02-10 18:30:04: alarm-notify.sh: INFO: sent discord notification for: serranito test.chart.  test_alarm is CLEAR to 'alarms'\n  2020-02-10 18:30:04: alarm-notify.sh: INFO: sent email notification for: serranito test.chart.  test_alarm is CLEAR to 'root'\n  # OK\n\n\nComo podemos ver, nos llega las alertas del test.\n\n\n",
        "url": "/sistemas/2020/02/11/Metricas_logs_y_monitorizacion_Netdata/"
      },
    
      {
        "title": "VPN con OpenVPN y certificados x509",
        "excerpt": "Vamos a establecer una VPN de acceso remoto y a establecer una VPN de sitio a sitio, (Autenticación será con TLS utilizando certificados X.509 y la asignación de direcciones dinámica).\n",
        "content": "\n\nVamos a establecer una VPN de acceso remoto y a establecer una VPN de sitio a sitio, (Autenticación será con TLS utilizando certificados X.509 y la asignación de direcciones dinámica).\n\nVPN de acceso remoto con OpenVPN y certificados x509\n\nConfigura una conexión VPN de acceso remoto entre dos equipos:\n\n\n  Uno de los dos equipos (el que actuará como servidor) estará conectado a dos redes.\n  Para la autenticación de los extremos se usarán obligatoriamente certificados digitales, que se generarán utilizando openssl y se almacenarán en el directorio /etc/openvpn, junto con los parámetros Diffie-Helman y el certificado de la propia Autoridad de Certificación.\n  Se utilizarán direcciones de la red 10.99.99.0/24 para las direcciones virtuales de la VPN. La dirección 10.99.99.1 se asignará al servidor VPN.\n  Los ficheros de configuración del servidor y del cliente se crearán en el directorio /etc/openvpn de cada máquina, y se llamarán servidor.conf y cliente.conf respectivamente. La configuración establecida debe cumplir los siguientes aspectos:\n    \n      El demonio openvpn se manejará con systemctl.\n      Se debe configurar para que la comunicación esté comprimida.\n      La asignación de direcciones IP será dinámica.\n      Existirá un fichero de log en el equipo.\n      Se mandarán a los clientes las rutas necesarias.\n    \n  \n  \n    Tras el establecimiento de la VPN, la máquina cliente debe ser capaz de acceder a una máquina que esté en la otra red a la que está conectado el servidor.\n  \n\n\nConfiguración Vagrant Cliente\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define :cliente1 do |cliente1|\n    cliente1.vm.box = \"debian/buster64\"\n    cliente1.vm.hostname = \"Cliente1\"\n    cliente1.vm.network :public_network,:bridge=&gt;\"enp3s0\"\n  end\nend\n\n\nConfiguración Vagrant Servidor\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define :servidor2 do |servidor2|\n    servidor2.vm.box = \"debian/buster64\"\n    servidor2.vm.hostname = \"Servidor2\"\n    servidor2.vm.network :public_network,:bridge=&gt;\"enp3s0\"\n    servidor2.vm.network :private_network, ip: \"192.168.100.1\", virtualbox__intnet:\"mired1\"\n  end\n  config.vm.define :cliente2 do |cliente2|\n    cliente2.vm.box = \"debian/buster64\"\n    cliente2.vm.hostname = \"Cliente2\"\n    cliente2.vm.network :private_network, ip: \"192.168.100.2\", virtualbox__intnet:\"mired1\"\n  end\nend\n\n\n\n  NOTA: Para esta tarea vamos a necesitar crear una Autoridad Certificadora y certificados firmados, CLIC AQUÍ para saber más.\n\n  Cliente1\n\n\nInstalamos el paquete openvpn para realizar la conexión a la vpn del servidor.\nsudo apt install openvpn\n\n\nCreamos los datos iguales que la autoridad certificadora\nCountry Name (2 letter code) [AU]:ES\nState or Province Name (full name) [Some-State]:Sevilla\nLocality Name (eg, city) []:Dos Hermanas\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:IES Gonzalo Nazareno\nOrganizational Unit Name (eg, section) []:Paloma \nCommon Name (e.g. server FQDN or YOUR name) []:Alejandro Morales Gracia\nEmail Address []:ale95mogra@gmail.com\t\n\n\nPasamos el fichero VPN_Alejandro.csr a la Autoridad Certificadora y esperamos a que nos devuelva el certificado firmado y el certificado de CA.\n\nAdemas vamos a mover los certificados y la clave privada a la ruta /etc/openvpn/client\n\nServidor2\n\nInstalamos el paquete openvpn para poder realizar la tarea.\nsudo apt install openvpn\n\n\nCreamos la clave Diffie-Hellman con el parámetro dhparam:\ncd /etc/openvpn\nsudo openssl dhparam -out dhparams.pem 4096\n\n\nActivamos el bit de forward para permitir el tráfico a través de el:\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\n\n\nSe edita el fichero /etc/openvpn/servidor.conf:\ndev tun\nserver 10.99.99.0 255.255.255.0 \npush \"route 192.168.100.0 255.255.255.0\"\ntls-server\ndh /etc/openvpn/dhparams.pem\nca /etc/openvpn/ca.cert.pem\ncert /etc/openvpn/serverVPN.crt \nkey /etc/openvpn/serverVPN.key\ncomp-lzo\nkeepalive 10 60\nverb 3\naskpass pass.txt\n\n\nVamos a firmar el certificado del cliente y se lo pasaremos para que pueda configurar su máquina.\nopenssl x509 -req -in VPN_Alejandro.csr -CA /root/ca/certs/ca.cert.pem -CAkey /root/ca/private/ca.key.pem -CAcreateserial -out VPN_Alejandro.crt\n   Signature ok\n   subject=C = ES, ST = Sevilla, L = Dos Hermanas, O = IES Gonzalo Nazareno, CN = Alejandro Morales Gracia\n   Getting CA Private Key\n   Enter pass phrase for /root/ca/private/ca.key.pem:\n\n\nCreamos un fichero llamado pass.txt y metemos una contraseña para que lo reconozca el parametro askpass.\nsudo touch pass.txt\necho \"prueba\" &gt; pass.txt\n\n\nConfiguramos openvpn, reconozca systemd, los ficheros de configuración. Esto se hace descomentando, la siguiente linea, en el fichero /etc/default/openvpn.\nAUTOSTART=\"all\"\n\n\nReiniciamos el servicio openvpn.service\nsudo systemctl restart openvpn.service\n\n\nCliente2\n\nEliminamos la tabla de enrrutmiento por defecto y le asignamos la dirección del Servidor2.\nsudo ip r del default\nsudo ip r add default via 192.168.100.20\n\n\nCliente1\n\nUna vez que tenemos los dos certificados, tenemos que hacer el fichero de configuración del cliente /etc/openvpn/VPN.conf, donde vamos a indicar la interfaz tun, la dirección a la que se tiene que conectar y los certificados.\ndev tun\nremote 172.22.0.56\nifconfig 10.99.99.0 255.255.255.0\npull\ntls-client\nca /etc/openvpn/client/ca.cert.pem\ncert /etc/openvpn/client/VPN_Alejandro.crt\nkey /etc/openvpn/client/VPN_Alejandro.key\ncomp-lzo\nkeepalive 10 60\nverb 3\naskpass pass.txt\n\n\nCreamos en un fichero llamado pass.txt y metemos una contraseña.\nsudo touch pass.txt\necho \"prueba\" &gt; pass.txt\n\n\nConfiguramos para que reconozca systemd los ficheros de configuración. Esto se hacer descomentando la siguiente linea en el fichero /etc/default/openvpn.\nAUTOSTART=\"all\"\n\n\nReiniciamos el servicio openvpn.service\nsudo systemctl restart openvpn.service\n\n\nComprobamos que se ha añadido el tun\nip a\n    1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n        inet 127.0.0.1/8 scope host lo\n           valid_lft forever preferred_lft forever\n        inet6 ::1/128 scope host \n           valid_lft forever preferred_lft forever\n    2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n        link/ether 08:00:27:8d:c0:4d brd ff:ff:ff:ff:ff:ff\n        inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0\n           valid_lft 85611sec preferred_lft 85611sec\n        inet6 fe80::a00:27ff:fe8d:c04d/64 scope link \n           valid_lft forever preferred_lft forever\n    3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n        link/ether 08:00:27:b8:70:44 brd ff:ff:ff:ff:ff:ff\n        inet 172.22.0.163/16 brd 172.22.255.255 scope global dynamic eth1\n           valid_lft 1029sec preferred_lft 1029sec\n        inet6 fe80::a00:27ff:feb8:7044/64 scope link \n           valid_lft forever preferred_lft forever\n    4: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group    default qlen 100\n        link/none \n        inet 10.99.99.6 peer 10.99.99.5/32 scope global tun0\n           valid_lft forever preferred_lft forever\n        inet6 fe80::9e0e:4d6c:ad95:ab79/64 scope link stable-privacy \n           valid_lft forever preferred_lft forever\n\nip r\n    default via 10.0.2.2 dev eth0 \n    10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 \n    10.99.99.1 via 10.99.99.5 dev tun0 \n    10.99.99.5 dev tun0 proto kernel scope link src 10.99.99.6 \n    172.22.0.0/16 dev eth1 proto kernel scope link src 172.22.2.117 \n    192.168.100.0/24 via 10.99.99.5 dev tun0 \n\n\nRealizamos un ping a la dirección 192.168.100.2 que es el cliente que tiene el servidor.\nping 192.168.100.2\n    PING 192.168.100.2 (192.168.100.2) 56(84) bytes of data.\n    64 bytes from 192.168.100.2: icmp_seq=1 ttl=64 time=1.28 ms\n    64 bytes from 192.168.100.2: icmp_seq=2 ttl=64 time=1.43 ms\n    64 bytes from 192.168.100.2: icmp_seq=3 ttl=64 time=1.34 ms\n    64 bytes from 192.168.100.2: icmp_seq=4 ttl=64 time=0.970 ms\n    64 bytes from 192.168.100.2: icmp_seq=5 ttl=64 time=1.39 ms\n    ^C\n    --- 192.168.100.2 ping statistics ---\n    5 packets transmitted, 5 received, 0% packet loss, time 12ms\n    rtt min/avg/max/mdev = 0.970/1.280/1.425/0.166 ms\n\n\nVPN sitio a sitio con OpenVPN y certificados x509\n\nConfigura una conexión VPN sitio a sitio entre dos equipos del cloud:\n\n\n  Cada equipo estará conectado a dos redes, una de ellas en común\n  Para la autenticación de los extremos se usarán obligatoriamente certificados digitales, que se generarán utilizando openssl y se almacenarán en el directorio /etc/openvpn, junto con con los parámetros Diffie-Helman y el certificado de la propia Autoridad de Certificación.\n  Se utilizarán direcciones de la red 10.99.99.0/24 para las direcciones virtuales de la VPN.\n  Tras el establecimiento de la VPN, una máquina de cada red detrás de cada servidor VPN debe ser capaz de acceder a una máquina del otro extremo.\n\n\nConfiguración Vagrant Servidor/Cliente\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define :servidor1 do |servidor1|\n    servidor1.vm.box = \"debian/buster64\"\n    servidor1.vm.hostname = \"Servidor/Cliente1\"\n    servidor1.vm.network :public_network,:bridge=&gt;\"enp3s0\"\n    servidor1.vm.network :private_network, ip: \"192.168.200.1\", virtualbox__intnet:\"mired1\"\n  end\n  config.vm.define :cliente1 do |cliente1|\n    cliente1.vm.box = \"debian/buster64\"\n    cliente1.vm.hostname = \"Cliente1\"\n    cliente1.vm.network :private_network, ip: \"192.168.200.2\", virtualbox__intnet:\"mired1\"\n  end\nend\n\n\nConfiguración Vagrant Servidor\nVagrant.configure(\"2\") do |config|\n\n  config.vm.define :servidor2 do |servidor2|\n    servidor2.vm.box = \"debian/buster64\"\n    servidor2.vm.hostname = \"Servidor2\"\n    servidor2.vm.network :public_network,:bridge=&gt;\"enp3s0\"\n    servidor2.vm.network :private_network, ip: \"192.168.100.20\", virtualbox__intnet:\"mired1\"\n  end\n  config.vm.define :cliente2 do |cliente2|\n    cliente2.vm.box = \"debian/buster64\"\n    cliente2.vm.hostname = \"Cliente2\"\n    cliente2.vm.network :private_network, ip: \"192.168.100.50\", virtualbox__intnet:\"mired1\"\n  end\nend\n\n\n\n  NOTA: Utilizaremos las mismas claves que en la anterior tarea.\n\n\nServidor2\n\nLa configuración para hacer una VPN Site to Site es igual que en la anterior tarea, pero el fichero servidor.conf cambia un poco.\n\nModificamos fichero de configuración del servidor servidor.conf.\ndev tun\nifconfig 10.99.99.1 10.99.99.2\nroute 192.168.200.0 255.255.255.0\ntls-server\ndh /etc/openvpn/dhparams.pem\nca /etc/openvpn/ca.cert.pem\ncert /etc/openvpn/serverVPN.crt\nkey /etc/openvpn/serverVPN.key\ncomp-lzo\nkeepalive 10 60\nlog /var/log/server.log\nverb 6\naskpass pass.txt\n\n\nComprobamos que el tunel se ha creado correctamente.\nip a\n    1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n        inet 127.0.0.1/8 scope host lo\n           valid_lft forever preferred_lft forever\n        inet6 ::1/128 scope host \n           valid_lft forever preferred_lft forever\n    2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n        link/ether 08:00:27:8d:c0:4d brd ff:ff:ff:ff:ff:ff\n        inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0\n           valid_lft 80741sec preferred_lft 80741sec\n        inet6 fe80::a00:27ff:fe8d:c04d/64 scope link \n           valid_lft forever preferred_lft forever\n    3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n        link/ether 08:00:27:90:f5:5a brd ff:ff:ff:ff:ff:ff\n        inet 172.22.9.60/16 brd 172.22.255.255 scope global dynamic eth1\n           valid_lft 9168sec preferred_lft 9168sec\n        inet6 fe80::a00:27ff:fe90:f55a/64 scope link \n           valid_lft forever preferred_lft forever\n    4: eth2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n        link/ether 08:00:27:4c:eb:52 brd ff:ff:ff:ff:ff:ff\n        inet 192.168.100.20/24 brd 192.168.100.255 scope global eth2\n           valid_lft forever preferred_lft forever\n        inet6 fe80::a00:27ff:fe4c:eb52/64 scope link \n           valid_lft forever preferred_lft forever\n    31: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group   default qlen 100\n        link/none \n        inet 10.99.99.1 peer 10.99.99.2/32 scope global tun0\n           valid_lft forever preferred_lft forever\n        inet6 fe80::dd5:e8c6:5fd1:f19c/64 scope link stable-privacy \n           valid_lft forever preferred_lft forever\n\n\nServidor/Cliente1\n\nAntes de configurar el cliente, el servidor tiene que haberle pasado al cliente los fichero VPN_Alejandro.key, VPN_Alejandro.crt y ca.cert.pem como en la anterior práctica.\n\nAhora podemos crear el fichero de configuración del cliente cliente.conf.\n\ndev tun\nifconfig 10.99.99.2 10.99.99.1\nremote 172.22.0.56\nroute 192.168.100.0 255.255.255.0\ntls-client\nca /etc/openvpn/client/ca.cert.pem\ncert /etc/openvpn/client/VPN_Alejandro.crt\nkey /etc/openvpn/client/VPN_Alejandro.key\ncomp-lzo\nkeepalive 10 60\nlog /var/log/host.log\nverb 6\naskpass pass.txt\n\n\nSe activa el bit de forward para dejar pasar el tráfico al Cliente1:\nroot@servidor:/home/vagrant# echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n\n\nReiniciamos el servicio\nsudo systemctl restart openvpn\n\n\nComprobamos que el tunel se ha creado correctamente y tenemos el nuevo registro de enrrutamiento.\nip a\n    1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n        inet 127.0.0.1/8 scope host lo\n           valid_lft forever preferred_lft forever\n        inet6 ::1/128 scope host \n           valid_lft forever preferred_lft forever\n    2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n        link/ether 08:00:27:8d:c0:4d brd ff:ff:ff:ff:ff:ff\n        inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0\n           valid_lft 84637sec preferred_lft 84637sec\n        inet6 fe80::a00:27ff:fe8d:c04d/64 scope link \n           valid_lft forever preferred_lft forever\n    3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n        link/ether 08:00:27:b8:70:44 brd ff:ff:ff:ff:ff:ff\n        inet 172.22.2.123/16 brd 172.22.255.255 scope global dynamic eth1\n           valid_lft 8996sec preferred_lft 8996sec\n        inet6 fe80::a00:27ff:feb8:7044/64 scope link \n           valid_lft forever preferred_lft forever\n    4: eth2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n        link/ether 08:00:27:7c:e2:d3 brd ff:ff:ff:ff:ff:ff\n        inet 192.168.200.1/24 brd 192.168.200.255 scope global eth2\n           valid_lft forever preferred_lft forever\n        inet6 fe80::a00:27ff:fe7c:e2d3/64 scope link \n           valid_lft forever preferred_lft forever\n    21: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 100\n        link/none \n        inet 10.99.99.2 peer 10.99.99.1/32 scope global tun0\n           valid_lft forever preferred_lft forever\n        inet6 fe80::83d:90fb:c65e:93d6/64 scope link stable-privacy \n           valid_lft forever preferred_lft forever\n\nip r\n    default via 10.0.2.2 dev eth0 \n    10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 \n    10.99.99.1 dev tun0 proto kernel scope link src 10.99.99.2 \n    172.22.0.0/16 dev eth1 proto kernel scope link src 172.22.2.123 \n    192.168.200.0/24 dev eth2 proto kernel scope link src 192.168.200.1 \n\n\nRealizamos un PING al Servidor2 y al Cliente2.\nvagrant@Servidor/Cliente1:/etc/openvpn$ ping 10.99.99.1\n    PING 10.99.99.1 (10.99.99.1) 56(84) bytes of data.\n    64 bytes from 10.99.99.1: icmp_seq=1 ttl=64 time=2.85 ms\n    64 bytes from 10.99.99.1: icmp_seq=2 ttl=64 time=3.33 ms\n    64 bytes from 10.99.99.1: icmp_seq=3 ttl=64 time=4.24 ms\n    ^C\n    --- 10.99.99.1 ping statistics ---\n    3 packets transmitted, 3 received, 0% packet loss, time 5ms\n    rtt min/avg/max/mdev = 2.851/3.473/4.240/0.576 ms\n\nvagrant@Servidor/Cliente1:/etc/openvpn$ ping 192.168.100.50\n    PING 192.168.100.50 (192.168.100.50) 56(84) bytes of data.\n    64 bytes from 192.168.100.50: icmp_seq=1 ttl=63 time=3.37 ms\n    64 bytes from 192.168.100.50: icmp_seq=2 ttl=63 time=8.10 ms\n    64 bytes from 192.168.100.50: icmp_seq=3 ttl=63 time=20.4 ms\n    ^C\n    --- 192.168.100.50 ping statistics ---\n    3 packets transmitted, 3 received, 0% packet loss, time 4ms\n    rtt min/avg/max/mdev = 3.367/10.631/20.428/7.191 ms\n\n\nCliente 1\n\nEliminamos la tabla de enrrutmiento por defecto y le asignamos la dirección del Servidor/Cliente1.\nsudo ip r del default\nsudo ip r add default via 192.168.200.1\n\n\nRealizamos un PING al Cliente2 del Servidor2\nvagrant@Cliente1:~$ ping 192.168.100.50\n    PING 192.168.100.50 (192.168.100.50) 56(84) bytes of data.\n    64 bytes from 192.168.100.50: icmp_seq=1 ttl=62 time=6.53 ms\n    64 bytes from 192.168.100.50: icmp_seq=2 ttl=62 time=3.73 ms\n    64 bytes from 192.168.100.50: icmp_seq=3 ttl=62 time=7.20 ms\n    ^C\n    --- 192.168.100.50 ping statistics ---\n    3 packets transmitted, 3 received, 0% packet loss, time 5ms\n    rtt min/avg/max/mdev = 3.726/5.819/7.199/1.506 ms\n\n\nCliente 2\n\nEliminamos la tabla de enrrutmiento por defecto y le asignamos la dirección del Servidor2.\nsudo ip r del default\nsudo ip r add default via 192.168.100.20\n\n\nRealizamos un PING al Cliente1 del Servidor/Cliente1\nvagrant@Cliente2:~$ ping 192.168.100.2\n    PING 192.168.100.2 (192.168.100.2) 56(84) bytes of data.\n    64 bytes from 192.168.100.2: icmp_seq=1 ttl=62 time=6.53 ms\n    64 bytes from 192.168.100.2: icmp_seq=2 ttl=62 time=3.73 ms\n    64 bytes from 192.168.100.2: icmp_seq=3 ttl=62 time=7.20 ms\n    ^C\n    --- 192.168.100.2 ping statistics ---\n    3 packets transmitted, 3 received, 0% packet loss, time 5ms\n    rtt min/avg/max/mdev = 3.726/5.819/7.199/1.506 ms\n\n",
        "url": "/seguridad/2020/02/11/VPN_con_OpenVPN_y_Certificadosx509/"
      },
    
  
  
  
  {
    "title": "Categorias",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "",
    "excerpt": "\n",
    "content": "\n\nMe llamo Alejandro Morales Gracia, nací en Dos Hermanas(Sevilla) 4 de Abril de 1995 y desde chico me han encantado las electricidad, electronica y los ordenadores.\n\nCuando terminé la ESO, estaba en duda, no sabía si dedicarme al mundo de la Electricidad o al de la Informática.\n\nAl final decidí que todo el tema de la electricidad y la automatización me gustaba un poco más y me metí de lleno en el Grado Medio de Instalaciones Electricas y Automáticas. Paso el tiempo y completé el curso y llegarón las prácicas; estas las hice en el norte de Italia, gracias a una beca Erasmus+ que me concedieron. Al terminar las prácticas, empece a trabajar en una empresa llamada ACA, donde estuve desempeñando la labor de Electromecánico y al mes se me acabo el contrato de Obras y Servicios, luego de esta, pasarón tres empresa con el mismo contrato, al cabo de dos años decidí que quería una vida más estable y me apunte al Grado Superior de Administración de Sistemas Informáticos en Red.\n",
    "url": "/elements/"
  },
  
  {
    "title": "",
    "excerpt": "\n",
    "content": "\n",
    "url": "/"
  },
  
  {
    "title": "Buscador",
    "excerpt": "Busca categorias, proyectos, fechas…\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  }
  
]

